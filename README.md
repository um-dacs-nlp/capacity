# Capacity: Measuring Transformer Memorization on Real-World Data

This repository contains the code, experiments, and visualizations for the paper:

**"Capacity Matters: a Proof-of-Concept for Transformer Memorization on Real-World Data"**
*Changalidi Anton, Aki H√§rm√§*
Accepted to **ACL 2025 Workshop L2M2: The First Workshop on Large Language Model Memorization**, Vienna, August 1st, 2025.

The link to the paper: <tbd>

---

## Overview

This project evaluates how decoder-only transformer architectures memorize real-world structured data. We use subsets of the **SNOMED CT** medical knowledge graph to construct two types of datasets:

* **Triplets**: (Concept, Property, Related Concept)
* **Sequences**: Simulated graph walks encoding multi-hop relationships

The experiments assess the influence of:

* Dataset size
* Number of layers
* Activation functions
* Embedding size (total number of parameters)

Using **Maximum Attainable Capacity (MAC)** as a key metric, the study identifies optimal architecture‚Äìdata trade-offs for memory-limited scenarios (e.g., edge devices).

---

## Repository Structure

```bash
.
‚îú‚îÄ‚îÄ img/                  # Figures and plots used in the paper
‚îú‚îÄ‚îÄ src/                 # Source code and experiments
‚îÇ   ‚îú‚îÄ‚îÄ data_management/     # Dataset extraction and preprocessing (requires Python 3.7)
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ 0_snomed_starting.ipynb     # Initialize and load SNOMED ontology
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ 1_snomed_get_dataset.ipynb  # Extract triplet datasets
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ 2_unlinked_construction.py  # Generate graph-walk sequences
‚îÇ   ‚îî‚îÄ‚îÄ transformers/         # Experiments and architecture evaluation
‚îÇ       ‚îú‚îÄ‚îÄ 1_iter_trans_big.ipynb               # Initial experiments (triplets)
‚îÇ       ‚îú‚îÄ‚îÄ 2_layers_activations*.ipynb/.py      # Layer & activation comparisons
‚îÇ       ‚îú‚îÄ‚îÄ 3_param_size*.ipynb/.py              # Parameter size impact
‚îÇ       ‚îî‚îÄ‚îÄ 4_seqs*.ipynb/.py                    # Experiments on sequence-based datasets
‚îú‚îÄ‚îÄ LICENSE
‚îú‚îÄ‚îÄ README.md
‚îî‚îÄ‚îÄ requirements.txt
```

---

## üìä Figures

All plots summarizing experimental results are located in `img/`. The filenames follow the format:

* `1_datasize_*` ‚Äî Results of Experiment 1 (dataset size)
* `2_layact_*` ‚Äî Experiment 2 (layer + activation variations)
* `3_paramsize_*` ‚Äî Experiment 3 (embedding/parameter size)
* `4_seqs_*` ‚Äî Experiment 4 (sequences)

Each comes in both `.pdf` and `.png` formats.

---

## üìö Setup Instructions

**Dependencies:**

```bash
pip install -r requirements.txt
```

> ‚ö†Ô∏è For the `data_management` part, **Python 3.7** is required due to dependency on `owlready2`.

You will need access to the SNOMED CT ontology file to run the data processing notebooks.

---

## üß™ Reproducing Experiments

Each experiment is defined as follows:

1. **Triplets ‚Äì Dataset Size Impact**
   File: `1_iter_trans_big.ipynb`
   Tests different dataset sizes with a fixed 1-layer transformer.

2. **Triplets ‚Äì Layer & Activation Influence**
   Files: `2_layers_activations_*`
   Tests ReLU, GELU, RReLU, Softmax with 1/2/4 layers.

3. **Triplets ‚Äì Parameter Size**
   Files: `3_param_size_*`
   Tests different embedding sizes for fixed parameter budgets.

4. **Sequences ‚Äì Graph Path Memorization**
   Files: `4_seqs_*`
   Tests ability to memorize sequences with 4‚Äì6 nodes.

---

## üß† Key Findings

* **Embedding size** is the dominant factor in capacity and speed.
* **Softmax** activation outperforms others in stability and accuracy.
* More layers do **not** improve performance on simple datasets.
* Sequences encode more structure and yield better memorization rates.
* MAC is an efficient alternative to full-scale MLS computation.

---

## üì¢ Citation

If you use this code or build upon this work, please cite:

(currently, this repository)

```
TBD
```

---

Let me know if you'd like a shortened or LaTeX version too.


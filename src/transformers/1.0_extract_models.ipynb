{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7aa14fe5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-27T20:40:33.738098Z",
     "iopub.status.busy": "2024-10-27T20:40:33.737725Z",
     "iopub.status.idle": "2024-10-27T20:40:35.985779Z",
     "shell.execute_reply": "2024-10-27T20:40:35.985151Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The nb_black extension is already loaded. To reload it, use:\n",
      "  %reload_ext nb_black\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 10;\n",
       "                var nbb_unformatted_code = \"%load_ext nb_black\\n\\nimport pickle\\nimport random\\n\\nimport matplotlib.pyplot as plt\\nimport numpy as np\\nimport pandas as pd\\nimport seaborn as sns\\nfrom collections import defaultdict\\nfrom datetime import datetime\\nfrom pprint import pprint\\nfrom tqdm import tqdm\\n\\nimport torch\\nimport torch.nn as nn\\nimport torch.optim as optim\\nfrom torch.utils.data import DataLoader, Dataset\\n\\nDATA = \\\"/home/I6356345/project/data/triplets.tsv\\\"\\nDATA_TOKENIZED_TO = \\\"/home/I6356345/project/data/tokenized_100k.tsv\\\"\\nSEED = 566\";\n",
       "                var nbb_formatted_code = \"%load_ext nb_black\\n\\nimport pickle\\nimport random\\n\\nimport matplotlib.pyplot as plt\\nimport numpy as np\\nimport pandas as pd\\nimport seaborn as sns\\nfrom collections import defaultdict\\nfrom datetime import datetime\\nfrom pprint import pprint\\nfrom tqdm import tqdm\\n\\nimport torch\\nimport torch.nn as nn\\nimport torch.optim as optim\\nfrom torch.utils.data import DataLoader, Dataset\\n\\nDATA = \\\"/home/I6356345/project/data/triplets.tsv\\\"\\nDATA_TOKENIZED_TO = \\\"/home/I6356345/project/data/tokenized_100k.tsv\\\"\\nSEED = 566\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%load_ext nb_black\n",
    "\n",
    "import pickle\n",
    "import random\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from collections import defaultdict\n",
    "from datetime import datetime\n",
    "from pprint import pprint\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "DATA = \"/home/I6356345/project/data/triplets.tsv\"\n",
    "DATA_TOKENIZED_TO = \"/home/I6356345/project/data/tokenized_100k.tsv\"\n",
    "SEED = 566"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3ae0c7b1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-27T20:40:35.989031Z",
     "iopub.status.busy": "2024-10-27T20:40:35.988621Z",
     "iopub.status.idle": "2024-10-27T20:40:36.231439Z",
     "shell.execute_reply": "2024-10-27T20:40:36.230793Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 2;\n",
       "                var nbb_unformatted_code = \"# Fix random seeds for reproducibility\\nrandom.seed(SEED)\\nnp.random.seed(SEED)\\ntorch.manual_seed(SEED)\\nif torch.cuda.is_available():\\n    torch.cuda.manual_seed(SEED)\\n    torch.cuda.manual_seed_all(SEED)\\ntorch.backends.cudnn.deterministic = True\\ntorch.backends.cudnn.benchmark = False\";\n",
       "                var nbb_formatted_code = \"# Fix random seeds for reproducibility\\nrandom.seed(SEED)\\nnp.random.seed(SEED)\\ntorch.manual_seed(SEED)\\nif torch.cuda.is_available():\\n    torch.cuda.manual_seed(SEED)\\n    torch.cuda.manual_seed_all(SEED)\\ntorch.backends.cudnn.deterministic = True\\ntorch.backends.cudnn.benchmark = False\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Fix random seeds for reproducibility\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(SEED)\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45cbbcbc",
   "metadata": {},
   "source": [
    "## Read and tokenize dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3bdde306",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-27T20:40:36.234958Z",
     "iopub.status.busy": "2024-10-27T20:40:36.234636Z",
     "iopub.status.idle": "2024-10-27T20:40:36.257827Z",
     "shell.execute_reply": "2024-10-27T20:40:36.257264Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 3;\n",
       "                var nbb_unformatted_code = \"def tokenize_columns(df, columns):\\n    # Create an empty vocabulary\\n    vocab = {}\\n    token_counter = 1  # Start token IDs from 1 (you can reserve 0 for padding if needed)\\n\\n    # Function to add unique column values to the vocab\\n    def add_to_vocab(value):\\n        nonlocal token_counter\\n        if value not in vocab:\\n            vocab[value] = token_counter\\n            token_counter += 1\\n\\n    # Add all unique values from the specified columns to the vocabulary\\n    for column in columns:\\n        df[column].apply(add_to_vocab)\\n\\n    # Function to tokenize a column value based on the vocab\\n    def tokenize(value):\\n        return [vocab[value]]  # Return token ID as a list to keep compatibility with batch processing\\n\\n    # Tokenize the specified columns\\n    for column in columns:\\n        df[f'tokenized_{column.lower().replace(\\\" \\\", \\\"_\\\")}'] = df[column].apply(tokenize)\\n\\n    # Combine tokenized concept and property into a single input sequence\\n    df['input_sequence'] = df.apply(lambda row: row['tokenized_concept'] + row['tokenized_property'], axis=1)\\n\\n    return df, vocab\\n\\n\\nclass TripletDataset(Dataset):\\n    def __init__(self, df):\\n        self.inputs = df['input_sequence'].tolist()\\n        self.targets = df['tokenized_related_concept'].tolist()\\n\\n    def __len__(self):\\n        return len(self.inputs)\\n\\n    def __getitem__(self, idx):\\n        input_sequence = torch.tensor(self.inputs[idx], dtype=torch.long)\\n        target_sequence = torch.tensor(self.targets[idx], dtype=torch.long)\\n        return input_sequence, target_sequence\\n\\n\\nclass GPTLikeModel(nn.Module):\\n    def __init__(self, vocab_size, d_model, n_heads, num_layers, max_seq_len, seed=42):\\n        super(GPTLikeModel, self).__init__()\\n        # Fix random seed for reproducibility\\n        torch.manual_seed(seed)\\n        if torch.cuda.is_available():\\n            torch.cuda.manual_seed(seed)\\n            torch.cuda.manual_seed_all(seed)\\n\\n        self.embedding = nn.Embedding(vocab_size, d_model)\\n        self.positional_encoding = nn.Parameter(torch.zeros(1, max_seq_len, d_model))\\n\\n        self.transformer_layers = nn.ModuleList(\\n            [\\n                nn.TransformerDecoderLayer(d_model=d_model, nhead=n_heads)\\n                for _ in range(num_layers)\\n            ]\\n        )\\n\\n        self.fc_out = nn.Linear(d_model, vocab_size)\\n\\n    def forward(self, x):\\n        # Embedding and positional encoding\\n        seq_len = x.size(1)\\n        x = self.embedding(x) + self.positional_encoding[:, :seq_len, :]\\n\\n        # Pass through each transformer decoder layer\\n        for layer in self.transformer_layers:\\n            x = layer(x, x)  # Decoder takes input twice in GPT-like models\\n\\n        # Output layer\\n        logits = self.fc_out(x)\\n        return logits\";\n",
       "                var nbb_formatted_code = \"def tokenize_columns(df, columns):\\n    # Create an empty vocabulary\\n    vocab = {}\\n    token_counter = (\\n        1  # Start token IDs from 1 (you can reserve 0 for padding if needed)\\n    )\\n\\n    # Function to add unique column values to the vocab\\n    def add_to_vocab(value):\\n        nonlocal token_counter\\n        if value not in vocab:\\n            vocab[value] = token_counter\\n            token_counter += 1\\n\\n    # Add all unique values from the specified columns to the vocabulary\\n    for column in columns:\\n        df[column].apply(add_to_vocab)\\n\\n    # Function to tokenize a column value based on the vocab\\n    def tokenize(value):\\n        return [\\n            vocab[value]\\n        ]  # Return token ID as a list to keep compatibility with batch processing\\n\\n    # Tokenize the specified columns\\n    for column in columns:\\n        df[f'tokenized_{column.lower().replace(\\\" \\\", \\\"_\\\")}'] = df[column].apply(tokenize)\\n\\n    # Combine tokenized concept and property into a single input sequence\\n    df[\\\"input_sequence\\\"] = df.apply(\\n        lambda row: row[\\\"tokenized_concept\\\"] + row[\\\"tokenized_property\\\"], axis=1\\n    )\\n\\n    return df, vocab\\n\\n\\nclass TripletDataset(Dataset):\\n    def __init__(self, df):\\n        self.inputs = df[\\\"input_sequence\\\"].tolist()\\n        self.targets = df[\\\"tokenized_related_concept\\\"].tolist()\\n\\n    def __len__(self):\\n        return len(self.inputs)\\n\\n    def __getitem__(self, idx):\\n        input_sequence = torch.tensor(self.inputs[idx], dtype=torch.long)\\n        target_sequence = torch.tensor(self.targets[idx], dtype=torch.long)\\n        return input_sequence, target_sequence\\n\\n\\nclass GPTLikeModel(nn.Module):\\n    def __init__(self, vocab_size, d_model, n_heads, num_layers, max_seq_len, seed=42):\\n        super(GPTLikeModel, self).__init__()\\n        # Fix random seed for reproducibility\\n        torch.manual_seed(seed)\\n        if torch.cuda.is_available():\\n            torch.cuda.manual_seed(seed)\\n            torch.cuda.manual_seed_all(seed)\\n\\n        self.embedding = nn.Embedding(vocab_size, d_model)\\n        self.positional_encoding = nn.Parameter(torch.zeros(1, max_seq_len, d_model))\\n\\n        self.transformer_layers = nn.ModuleList(\\n            [\\n                nn.TransformerDecoderLayer(d_model=d_model, nhead=n_heads)\\n                for _ in range(num_layers)\\n            ]\\n        )\\n\\n        self.fc_out = nn.Linear(d_model, vocab_size)\\n\\n    def forward(self, x):\\n        # Embedding and positional encoding\\n        seq_len = x.size(1)\\n        x = self.embedding(x) + self.positional_encoding[:, :seq_len, :]\\n\\n        # Pass through each transformer decoder layer\\n        for layer in self.transformer_layers:\\n            x = layer(x, x)  # Decoder takes input twice in GPT-like models\\n\\n        # Output layer\\n        logits = self.fc_out(x)\\n        return logits\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def tokenize_columns(df, columns):\n",
    "    # Create an empty vocabulary\n",
    "    vocab = {}\n",
    "    token_counter = 1  # Start token IDs from 1 (you can reserve 0 for padding if needed)\n",
    "\n",
    "    # Function to add unique column values to the vocab\n",
    "    def add_to_vocab(value):\n",
    "        nonlocal token_counter\n",
    "        if value not in vocab:\n",
    "            vocab[value] = token_counter\n",
    "            token_counter += 1\n",
    "\n",
    "    # Add all unique values from the specified columns to the vocabulary\n",
    "    for column in columns:\n",
    "        df[column].apply(add_to_vocab)\n",
    "\n",
    "    # Function to tokenize a column value based on the vocab\n",
    "    def tokenize(value):\n",
    "        return [vocab[value]]  # Return token ID as a list to keep compatibility with batch processing\n",
    "\n",
    "    # Tokenize the specified columns\n",
    "    for column in columns:\n",
    "        df[f'tokenized_{column.lower().replace(\" \", \"_\")}'] = df[column].apply(tokenize)\n",
    "\n",
    "    # Combine tokenized concept and property into a single input sequence\n",
    "    df['input_sequence'] = df.apply(lambda row: row['tokenized_concept'] + row['tokenized_property'], axis=1)\n",
    "\n",
    "    return df, vocab\n",
    "\n",
    "\n",
    "class TripletDataset(Dataset):\n",
    "    def __init__(self, df):\n",
    "        self.inputs = df['input_sequence'].tolist()\n",
    "        self.targets = df['tokenized_related_concept'].tolist()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.inputs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        input_sequence = torch.tensor(self.inputs[idx], dtype=torch.long)\n",
    "        target_sequence = torch.tensor(self.targets[idx], dtype=torch.long)\n",
    "        return input_sequence, target_sequence\n",
    "\n",
    "\n",
    "class GPTLikeModel(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model, n_heads, num_layers, max_seq_len, seed=42):\n",
    "        super(GPTLikeModel, self).__init__()\n",
    "        # Fix random seed for reproducibility\n",
    "        torch.manual_seed(seed)\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.manual_seed(seed)\n",
    "            torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
    "        self.positional_encoding = nn.Parameter(torch.zeros(1, max_seq_len, d_model))\n",
    "\n",
    "        self.transformer_layers = nn.ModuleList(\n",
    "            [\n",
    "                nn.TransformerDecoderLayer(d_model=d_model, nhead=n_heads)\n",
    "                for _ in range(num_layers)\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        self.fc_out = nn.Linear(d_model, vocab_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Embedding and positional encoding\n",
    "        seq_len = x.size(1)\n",
    "        x = self.embedding(x) + self.positional_encoding[:, :seq_len, :]\n",
    "\n",
    "        # Pass through each transformer decoder layer\n",
    "        for layer in self.transformer_layers:\n",
    "            x = layer(x, x)  # Decoder takes input twice in GPT-like models\n",
    "\n",
    "        # Output layer\n",
    "        logits = self.fc_out(x)\n",
    "        return logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "43aeb321",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-27T20:40:36.260737Z",
     "iopub.status.busy": "2024-10-27T20:40:36.260445Z",
     "iopub.status.idle": "2024-10-27T20:40:39.780506Z",
     "shell.execute_reply": "2024-10-27T20:40:39.779778Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Concept</th>\n",
       "      <th>Property</th>\n",
       "      <th>Related Concept</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1473772</th>\n",
       "      <td>ECG: U wave exaggerated</td>\n",
       "      <td>case_significance_id</td>\n",
       "      <td>900000000000017005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3289696</th>\n",
       "      <td>Talazoparib (as talazoparib tosylate) 1 mg ora...</td>\n",
       "      <td>definition_status_id</td>\n",
       "      <td>900000000000073002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>471179</th>\n",
       "      <td>Pentostatin-containing product</td>\n",
       "      <td>definition_status_id</td>\n",
       "      <td>900000000000073002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>670735</th>\n",
       "      <td>Fistulectomy of rectum</td>\n",
       "      <td>type_id</td>\n",
       "      <td>900000000000013009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>693462</th>\n",
       "      <td>Excision of neurofibroma of cutaneous nerve</td>\n",
       "      <td>has_direct_procedure_site</td>\n",
       "      <td>Peripheral nerve structure</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   Concept  \\\n",
       "1473772                            ECG: U wave exaggerated   \n",
       "3289696  Talazoparib (as talazoparib tosylate) 1 mg ora...   \n",
       "471179                      Pentostatin-containing product   \n",
       "670735                              Fistulectomy of rectum   \n",
       "693462         Excision of neurofibroma of cutaneous nerve   \n",
       "\n",
       "                          Property             Related Concept  \n",
       "1473772       case_significance_id          900000000000017005  \n",
       "3289696       definition_status_id          900000000000073002  \n",
       "471179        definition_status_id          900000000000073002  \n",
       "670735                     type_id          900000000000013009  \n",
       "693462   has_direct_procedure_site  Peripheral nerve structure  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 4;\n",
       "                var nbb_unformatted_code = \"# Load TSV data\\ndata = pd.read_csv(DATA, sep=\\\"\\\\t\\\")\\ndf = data.sample(n=100000, random_state=SEED)\\n# df = data[:100000]\\ndf.head()\";\n",
       "                var nbb_formatted_code = \"# Load TSV data\\ndata = pd.read_csv(DATA, sep=\\\"\\\\t\\\")\\ndf = data.sample(n=100000, random_state=SEED)\\n# df = data[:100000]\\ndf.head()\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load TSV data\n",
    "data = pd.read_csv(DATA, sep=\"\\t\")\n",
    "df = data.sample(n=100000, random_state=SEED)\n",
    "# df = data[:100000]\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f6fbbd57",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-27T20:40:39.783747Z",
     "iopub.status.busy": "2024-10-27T20:40:39.783408Z",
     "iopub.status.idle": "2024-10-27T20:40:41.688904Z",
     "shell.execute_reply": "2024-10-27T20:40:41.688233Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Concept</th>\n",
       "      <th>Property</th>\n",
       "      <th>Related Concept</th>\n",
       "      <th>tokenized_concept</th>\n",
       "      <th>tokenized_property</th>\n",
       "      <th>tokenized_related_concept</th>\n",
       "      <th>input_sequence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1473772</th>\n",
       "      <td>ECG: U wave exaggerated</td>\n",
       "      <td>case_significance_id</td>\n",
       "      <td>900000000000017005</td>\n",
       "      <td>[1]</td>\n",
       "      <td>[88028]</td>\n",
       "      <td>[88218]</td>\n",
       "      <td>[1, 88028]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3289696</th>\n",
       "      <td>Talazoparib (as talazoparib tosylate) 1 mg ora...</td>\n",
       "      <td>definition_status_id</td>\n",
       "      <td>900000000000073002</td>\n",
       "      <td>[2]</td>\n",
       "      <td>[88029]</td>\n",
       "      <td>[88219]</td>\n",
       "      <td>[2, 88029]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>471179</th>\n",
       "      <td>Pentostatin-containing product</td>\n",
       "      <td>definition_status_id</td>\n",
       "      <td>900000000000073002</td>\n",
       "      <td>[3]</td>\n",
       "      <td>[88029]</td>\n",
       "      <td>[88219]</td>\n",
       "      <td>[3, 88029]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>670735</th>\n",
       "      <td>Fistulectomy of rectum</td>\n",
       "      <td>type_id</td>\n",
       "      <td>900000000000013009</td>\n",
       "      <td>[4]</td>\n",
       "      <td>[88030]</td>\n",
       "      <td>[88220]</td>\n",
       "      <td>[4, 88030]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>693462</th>\n",
       "      <td>Excision of neurofibroma of cutaneous nerve</td>\n",
       "      <td>has_direct_procedure_site</td>\n",
       "      <td>Peripheral nerve structure</td>\n",
       "      <td>[5]</td>\n",
       "      <td>[88031]</td>\n",
       "      <td>[88221]</td>\n",
       "      <td>[5, 88031]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   Concept  \\\n",
       "1473772                            ECG: U wave exaggerated   \n",
       "3289696  Talazoparib (as talazoparib tosylate) 1 mg ora...   \n",
       "471179                      Pentostatin-containing product   \n",
       "670735                              Fistulectomy of rectum   \n",
       "693462         Excision of neurofibroma of cutaneous nerve   \n",
       "\n",
       "                          Property             Related Concept  \\\n",
       "1473772       case_significance_id          900000000000017005   \n",
       "3289696       definition_status_id          900000000000073002   \n",
       "471179        definition_status_id          900000000000073002   \n",
       "670735                     type_id          900000000000013009   \n",
       "693462   has_direct_procedure_site  Peripheral nerve structure   \n",
       "\n",
       "        tokenized_concept tokenized_property tokenized_related_concept  \\\n",
       "1473772               [1]            [88028]                   [88218]   \n",
       "3289696               [2]            [88029]                   [88219]   \n",
       "471179                [3]            [88029]                   [88219]   \n",
       "670735                [4]            [88030]                   [88220]   \n",
       "693462                [5]            [88031]                   [88221]   \n",
       "\n",
       "        input_sequence  \n",
       "1473772     [1, 88028]  \n",
       "3289696     [2, 88029]  \n",
       "471179      [3, 88029]  \n",
       "670735      [4, 88030]  \n",
       "693462      [5, 88031]  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 10 rows of Vocabulary:\n",
      "{'Acute Q wave infarction - anterolateral': 6,\n",
      " 'Blood group antibody P^k^': 8,\n",
      " 'Cluster of differentiation antigen 45 R': 10,\n",
      " 'ECG: U wave exaggerated': 1,\n",
      " 'Excision of neurofibroma of cutaneous nerve': 5,\n",
      " 'Fistulectomy of rectum': 4,\n",
      " 'Ondansetron-containing product in parenteral dose form': 9,\n",
      " 'Pentostatin-containing product': 3,\n",
      " 'Stromeyer-Little operation hepatotomy': 7,\n",
      " 'Talazoparib (as talazoparib tosylate) 1 mg oral capsule': 2}\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 5;\n",
       "                var nbb_unformatted_code = \"columns_to_tokenize = [\\\"Concept\\\", \\\"Property\\\", \\\"Related Concept\\\"]\\n\\ndf_tokenized, vocab = tokenize_columns(df, columns_to_tokenize)\\ndisplay(df.head())\\nprint(\\\"First 10 rows of Vocabulary:\\\")\\npprint(dict(list(vocab.items())[:10]))\";\n",
       "                var nbb_formatted_code = \"columns_to_tokenize = [\\\"Concept\\\", \\\"Property\\\", \\\"Related Concept\\\"]\\n\\ndf_tokenized, vocab = tokenize_columns(df, columns_to_tokenize)\\ndisplay(df.head())\\nprint(\\\"First 10 rows of Vocabulary:\\\")\\npprint(dict(list(vocab.items())[:10]))\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "columns_to_tokenize = [\"Concept\", \"Property\", \"Related Concept\"]\n",
    "\n",
    "df_tokenized, vocab = tokenize_columns(df, columns_to_tokenize)\n",
    "display(df.head())\n",
    "print(\"First 10 rows of Vocabulary:\")\n",
    "pprint(dict(list(vocab.items())[:10]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "793ad74b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 11;\n",
       "                var nbb_unformatted_code = \"df.to_csv(DATA_TOKENIZED_TO, sep='\\\\t', index=False)\";\n",
       "                var nbb_formatted_code = \"df.to_csv(DATA_TOKENIZED_TO, sep=\\\"\\\\t\\\", index=False)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df.to_csv(DATA_TOKENIZED_TO, sep=\"\\t\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad3077aa",
   "metadata": {},
   "source": [
    "# MODEL"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "432cd088",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-27T20:40:41.692244Z",
     "iopub.status.busy": "2024-10-27T20:40:41.691922Z",
     "iopub.status.idle": "2024-10-27T20:40:41.707718Z",
     "shell.execute_reply": "2024-10-27T20:40:41.707156Z"
    }
   },
   "source": [
    "## MODEL.SAVE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1dd93a60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "models_20241023_101847_init_trans.pkl\r\n",
      "models_20241023_101900_init_trans.pkl\r\n",
      "models_20241023_102044_init_trans.pkl\r\n",
      "models_20241025_153126_init_trans.pkl\r\n",
      "models_20241027_223358_init_trans.pkl\r\n",
      "models_20241030_002102_50_100k_trans.pkl\r\n",
      "models_20241030_020035_50_100k_trans.pkl\r\n",
      "models_20241030_035655_50_100k_trans.pkl\r\n",
      "models_20241030_061020_50_100k_trans.pkl\r\n",
      "models_20241030_084050_50_100k_trans.pkl\r\n",
      "models_20241030_112648_50_100k_trans.pkl\r\n",
      "models_20241105_062400_50_100k_trans.pkl\r\n",
      "models_20241105_183211_init_trans.pkl\r\n",
      "models_20241105_222648_50_100k_trans.pkl\r\n",
      "models_20241106_160815_50_100k_trans.pkl\r\n",
      "models_20241107_120404_50_100k_trans.pkl\r\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 15;\n",
       "                var nbb_unformatted_code = \"! ls ../../../data/out_models\";\n",
       "                var nbb_formatted_code = \"! ls ../../../data/out_models\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "! ls ../../../data/out_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0ef0d7ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPTLikeModel(\n",
      "  (embedding): Embedding(112457, 128)\n",
      "  (transformer_layers): ModuleList(\n",
      "    (0): TransformerDecoderLayer(\n",
      "      (self_attn): MultiheadAttention(\n",
      "        (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
      "      )\n",
      "      (multihead_attn): MultiheadAttention(\n",
      "        (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
      "      )\n",
      "      (linear1): Linear(in_features=128, out_features=2048, bias=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "      (linear2): Linear(in_features=2048, out_features=128, bias=True)\n",
      "      (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "      (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "      (norm3): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "      (dropout1): Dropout(p=0.1, inplace=False)\n",
      "      (dropout2): Dropout(p=0.1, inplace=False)\n",
      "      (dropout3): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "  )\n",
      "  (fc_out): Linear(in_features=128, out_features=112457, bias=True)\n",
      ")\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 18;\n",
       "                var nbb_unformatted_code = \"with open(\\n    \\\"../../../data/out_models/models_20241030_112648_50_100k_trans.pkl\\\", \\\"rb\\\"\\n) as file:\\n    models = pickle.load(file)\\nprint(models[100000])\";\n",
       "                var nbb_formatted_code = \"with open(\\n    \\\"../../../data/out_models/models_20241030_112648_50_100k_trans.pkl\\\", \\\"rb\\\"\\n) as file:\\n    models = pickle.load(file)\\nprint(models[100000])\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "with open(\n",
    "    \"../../../data/out_models/models_20241030_112648_50_100k_trans.pkl\", \"rb\"\n",
    ") as file:\n",
    "    models = pickle.load(file)\n",
    "print(models[100000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "94d995d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 19;\n",
       "                var nbb_unformatted_code = \"torch.save(models[100000].state_dict(), \\\"../../../data/out_models/model_20241030_112648_100k.pt\\\")\";\n",
       "                var nbb_formatted_code = \"torch.save(\\n    models[100000].state_dict(),\\n    \\\"../../../data/out_models/model_20241030_112648_100k.pt\\\",\\n)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "torch.save(\n",
    "    models[100000].state_dict(),\n",
    "    \"../../../data/out_models/model_20241030_112648_100k.pt\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae2fcc83",
   "metadata": {},
   "source": [
    "## MODEL.LOAD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "75101047",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPTLikeModel(\n",
       "  (embedding): Embedding(112457, 128)\n",
       "  (transformer_layers): ModuleList(\n",
       "    (0): TransformerDecoderLayer(\n",
       "      (self_attn): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
       "      )\n",
       "      (multihead_attn): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
       "      )\n",
       "      (linear1): Linear(in_features=128, out_features=2048, bias=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "      (linear2): Linear(in_features=2048, out_features=128, bias=True)\n",
       "      (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "      (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "      (norm3): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout1): Dropout(p=0.1, inplace=False)\n",
       "      (dropout2): Dropout(p=0.1, inplace=False)\n",
       "      (dropout3): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "  )\n",
       "  (fc_out): Linear(in_features=128, out_features=112457, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 23;\n",
       "                var nbb_unformatted_code = \"## then later:\\n# model hyperparameters\\nvocab_size = len(vocab) + 1  # Include 1 for padding (if needed)\\nd_model = 128  # Embedding size\\nn_heads = 4  # Number of attention heads\\nnum_layers = 1  # Number of transformer layers\\nmax_seq_len = 2  # Maximum sequence length (concept + property)\\nbatch_size = 64\\nlr = 0.001\\nmodel = GPTLikeModel(vocab_size, d_model, n_heads, num_layers, max_seq_len, seed=SEED)\\nmodel.load_state_dict(\\n    torch.load(\\\"../../../data/out_models/model_20241030_112648_100k.pt\\\")\\n)\\nmodel\";\n",
       "                var nbb_formatted_code = \"## then later:\\n# model hyperparameters\\nvocab_size = len(vocab) + 1  # Include 1 for padding (if needed)\\nd_model = 128  # Embedding size\\nn_heads = 4  # Number of attention heads\\nnum_layers = 1  # Number of transformer layers\\nmax_seq_len = 2  # Maximum sequence length (concept + property)\\nbatch_size = 64\\nlr = 0.001\\nmodel = GPTLikeModel(vocab_size, d_model, n_heads, num_layers, max_seq_len, seed=SEED)\\nmodel.load_state_dict(\\n    torch.load(\\\"../../../data/out_models/model_20241030_112648_100k.pt\\\")\\n)\\nmodel\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## then later:\n",
    "# model hyperparameters\n",
    "vocab_size = len(vocab) + 1  # Include 1 for padding (if needed)\n",
    "d_model = 128  # Embedding size\n",
    "n_heads = 4  # Number of attention heads\n",
    "num_layers = 1  # Number of transformer layers\n",
    "max_seq_len = 2  # Maximum sequence length (concept + property)\n",
    "batch_size = 64\n",
    "lr = 0.001\n",
    "model = GPTLikeModel(vocab_size, d_model, n_heads, num_layers, max_seq_len, seed=SEED)\n",
    "model.load_state_dict(\n",
    "    torch.load(\"../../../data/out_models/model_20241030_112648_100k.pt\")\n",
    ")\n",
    "model"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4894b06d",
   "metadata": {},
   "source": [
    "# Imports, constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7aa14fe5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-27T20:40:33.738098Z",
     "iopub.status.busy": "2024-10-27T20:40:33.737725Z",
     "iopub.status.idle": "2024-10-27T20:40:35.985779Z",
     "shell.execute_reply": "2024-10-27T20:40:35.985151Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The nb_black extension is already loaded. To reload it, use:\n",
      "  %reload_ext nb_black\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 11;\n",
       "                var nbb_unformatted_code = \"%load_ext nb_black\\n\\n# imports\\nimport pickle\\nimport random\\n\\nimport matplotlib.pyplot as plt\\nimport numpy as np\\nimport pandas as pd\\nimport seaborn as sns\\nfrom collections import defaultdict\\nfrom datetime import datetime\\nfrom pprint import pprint\\nfrom tqdm import tqdm\\nfrom copy import deepcopy\\nimport itertools\\nimport os\\n\\nimport torch\\nimport torch.nn as nn\\nimport torch.optim as optim\\nfrom torch.utils.data import DataLoader, Dataset\\n\\n\\n# constants\\nDATA = \\\"../../../data/triplets.tsv\\\"\\nSEED = 566\\n# Results and models file paths\\nRESULTS_FILE = (\\n    \\\"../../../data/out_metrics/results_{timestamp}_lay_act_{config_index}.pkl\\\"\\n)\\nMODELS_FILE = \\\"../../../data/out_models/models_{timestamp}_lay_act_{config_index}.pkl\\\"\\n\\n\\n# fix random seeds for reproducibility\\nrandom.seed(SEED)\\nnp.random.seed(SEED)\\ntorch.manual_seed(SEED)\\nif torch.cuda.is_available():\\n    torch.cuda.manual_seed(SEED)\\n    torch.cuda.manual_seed_all(SEED)\\ntorch.backends.cudnn.deterministic = True\\ntorch.backends.cudnn.benchmark = False\";\n",
       "                var nbb_formatted_code = \"%load_ext nb_black\\n\\n# imports\\nimport pickle\\nimport random\\n\\nimport matplotlib.pyplot as plt\\nimport numpy as np\\nimport pandas as pd\\nimport seaborn as sns\\nfrom collections import defaultdict\\nfrom datetime import datetime\\nfrom pprint import pprint\\nfrom tqdm import tqdm\\nfrom copy import deepcopy\\nimport itertools\\nimport os\\n\\nimport torch\\nimport torch.nn as nn\\nimport torch.optim as optim\\nfrom torch.utils.data import DataLoader, Dataset\\n\\n\\n# constants\\nDATA = \\\"../../../data/triplets.tsv\\\"\\nSEED = 566\\n# Results and models file paths\\nRESULTS_FILE = (\\n    \\\"../../../data/out_metrics/results_{timestamp}_lay_act_{config_index}.pkl\\\"\\n)\\nMODELS_FILE = \\\"../../../data/out_models/models_{timestamp}_lay_act_{config_index}.pkl\\\"\\n\\n\\n# fix random seeds for reproducibility\\nrandom.seed(SEED)\\nnp.random.seed(SEED)\\ntorch.manual_seed(SEED)\\nif torch.cuda.is_available():\\n    torch.cuda.manual_seed(SEED)\\n    torch.cuda.manual_seed_all(SEED)\\ntorch.backends.cudnn.deterministic = True\\ntorch.backends.cudnn.benchmark = False\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%load_ext nb_black\n",
    "\n",
    "# imports\n",
    "import pickle\n",
    "import random\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from collections import defaultdict\n",
    "from datetime import datetime\n",
    "from pprint import pprint\n",
    "from tqdm import tqdm\n",
    "from copy import deepcopy\n",
    "import itertools\n",
    "import os\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "\n",
    "# constants\n",
    "DATA = \"../../../data/triplets.tsv\"\n",
    "SEED = 566\n",
    "# Results and models file paths\n",
    "RESULTS_FILE = (\n",
    "    \"../../../data/out_metrics/results_{timestamp}_lay_act_{config_index}.pkl\"\n",
    ")\n",
    "MODELS_FILE = \"../../../data/out_models/models_{timestamp}_lay_act_{config_index}.pkl\"\n",
    "\n",
    "\n",
    "# fix random seeds for reproducibility\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(SEED)\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "491a4dbf",
   "metadata": {},
   "source": [
    "# All Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3ae0c7b1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-27T20:40:35.989031Z",
     "iopub.status.busy": "2024-10-27T20:40:35.988621Z",
     "iopub.status.idle": "2024-10-27T20:40:36.231439Z",
     "shell.execute_reply": "2024-10-27T20:40:36.230793Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 12;\n",
       "                var nbb_unformatted_code = \"# ===============================\\n# For data\\n# ===============================\\n\\n\\ndef tokenize_columns(df_in, columns):\\n    df = deepcopy(df_in)\\n    # Create an empty vocabulary\\n    vocab = {}\\n    token_counter = (\\n        1  # Start token IDs from 1 (you can reserve 0 for padding if needed)\\n    )\\n\\n    # Function to add unique column values to the vocab\\n    def add_to_vocab(value):\\n        nonlocal token_counter\\n        if value not in vocab:\\n            vocab[value] = token_counter\\n            token_counter += 1\\n\\n    # Add all unique values from the specified columns to the vocabulary\\n    for column in columns:\\n        df[column].apply(add_to_vocab)\\n\\n    # Function to tokenize a column value based on the vocab\\n    def tokenize(value):\\n        return [\\n            vocab[value]\\n        ]  # Return token ID as a list to keep compatibility with batch processing\\n\\n    # Tokenize the specified columns\\n    for column in columns:\\n        df[f'tokenized_{column.lower().replace(\\\" \\\", \\\"_\\\")}'] = df[column].apply(tokenize)\\n\\n    # Combine tokenized concept and property into a single input sequence\\n    df[\\\"input_sequence\\\"] = df.apply(\\n        lambda row: row[\\\"tokenized_concept\\\"] + row[\\\"tokenized_property\\\"], axis=1\\n    )\\n\\n    return df, vocab\\n\\n\\nclass TripletDataset(Dataset):\\n    def __init__(self, df):\\n        self.inputs = df[\\\"input_sequence\\\"].tolist()\\n        self.targets = df[\\\"tokenized_related_concept\\\"].tolist()\\n\\n    def __len__(self):\\n        return len(self.inputs)\\n\\n    def __getitem__(self, idx):\\n        input_sequence = torch.tensor(self.inputs[idx], dtype=torch.long)\\n        target_sequence = torch.tensor(self.targets[idx], dtype=torch.long)\\n        return input_sequence, target_sequence\\n\\n\\n# ===============================\\n# Model\\n# ===============================\\n\\n\\n# Define the custom activation functions\\ndef get_activation_function(name):\\n    if name == \\\"default\\\":\\n        return nn.ReLU()\\n    elif name == \\\"GELU\\\":\\n        return nn.GELU()\\n    elif name == \\\"RAF\\\":\\n        return nn.RReLU()\\n    elif name == \\\"softmax\\\":\\n        return nn.Softmax(dim=-1)\\n    else:\\n        raise ValueError(f\\\"Unknown activation function: {name}\\\")\\n\\n\\n# Modified model class to include dynamic activation function and adaptable hidden size\\nclass GPTLikeModel(nn.Module):\\n    def __init__(\\n        self,\\n        vocab_size,\\n        d_model,\\n        n_heads,\\n        num_layers,\\n        max_seq_len,\\n        activation_fn,\\n        seed=42,\\n    ):\\n        super(GPTLikeModel, self).__init__()\\n        # Fix random seed for reproducibility\\n        torch.manual_seed(seed)\\n        if torch.cuda.is_available():\\n            torch.cuda.manual_seed(seed)\\n            torch.cuda.manual_seed_all(seed)\\n\\n        self.embedding = nn.Embedding(vocab_size, d_model)\\n        self.positional_encoding = nn.Parameter(torch.zeros(1, max_seq_len, d_model))\\n\\n        self.transformer_layers = nn.ModuleList(\\n            [\\n                nn.TransformerDecoderLayer(\\n                    d_model=d_model, nhead=n_heads, activation=activation_fn\\n                )\\n                for _ in range(num_layers)\\n            ]\\n        )\\n\\n        self.fc_out = nn.Linear(d_model, vocab_size)\\n\\n    def forward(self, x):\\n        # Embedding and positional encoding\\n        seq_len = x.size(1)\\n        x = self.embedding(x) + self.positional_encoding[:, :seq_len, :]\\n\\n        # Pass through each transformer decoder layer\\n        for layer in self.transformer_layers:\\n            x = layer(x, x)  # Decoder takes input twice in GPT-like models\\n\\n        # Output layer\\n        logits = self.fc_out(x)\\n        return logits\\n\\n\\n# other logic\\n\\n\\n# Define a constant for the filename format\\n# Example function to save results\\ndef save_results(results, filename):\\n    with open(filename, \\\"wb\\\") as f:\\n        pickle.dump(results, f)\\n    print(f\\\"Results saved to {filename}\\\")\";\n",
       "                var nbb_formatted_code = \"# ===============================\\n# For data\\n# ===============================\\n\\n\\ndef tokenize_columns(df_in, columns):\\n    df = deepcopy(df_in)\\n    # Create an empty vocabulary\\n    vocab = {}\\n    token_counter = (\\n        1  # Start token IDs from 1 (you can reserve 0 for padding if needed)\\n    )\\n\\n    # Function to add unique column values to the vocab\\n    def add_to_vocab(value):\\n        nonlocal token_counter\\n        if value not in vocab:\\n            vocab[value] = token_counter\\n            token_counter += 1\\n\\n    # Add all unique values from the specified columns to the vocabulary\\n    for column in columns:\\n        df[column].apply(add_to_vocab)\\n\\n    # Function to tokenize a column value based on the vocab\\n    def tokenize(value):\\n        return [\\n            vocab[value]\\n        ]  # Return token ID as a list to keep compatibility with batch processing\\n\\n    # Tokenize the specified columns\\n    for column in columns:\\n        df[f'tokenized_{column.lower().replace(\\\" \\\", \\\"_\\\")}'] = df[column].apply(tokenize)\\n\\n    # Combine tokenized concept and property into a single input sequence\\n    df[\\\"input_sequence\\\"] = df.apply(\\n        lambda row: row[\\\"tokenized_concept\\\"] + row[\\\"tokenized_property\\\"], axis=1\\n    )\\n\\n    return df, vocab\\n\\n\\nclass TripletDataset(Dataset):\\n    def __init__(self, df):\\n        self.inputs = df[\\\"input_sequence\\\"].tolist()\\n        self.targets = df[\\\"tokenized_related_concept\\\"].tolist()\\n\\n    def __len__(self):\\n        return len(self.inputs)\\n\\n    def __getitem__(self, idx):\\n        input_sequence = torch.tensor(self.inputs[idx], dtype=torch.long)\\n        target_sequence = torch.tensor(self.targets[idx], dtype=torch.long)\\n        return input_sequence, target_sequence\\n\\n\\n# ===============================\\n# Model\\n# ===============================\\n\\n\\n# Define the custom activation functions\\ndef get_activation_function(name):\\n    if name == \\\"default\\\":\\n        return nn.ReLU()\\n    elif name == \\\"GELU\\\":\\n        return nn.GELU()\\n    elif name == \\\"RAF\\\":\\n        return nn.RReLU()\\n    elif name == \\\"softmax\\\":\\n        return nn.Softmax(dim=-1)\\n    else:\\n        raise ValueError(f\\\"Unknown activation function: {name}\\\")\\n\\n\\n# Modified model class to include dynamic activation function and adaptable hidden size\\nclass GPTLikeModel(nn.Module):\\n    def __init__(\\n        self,\\n        vocab_size,\\n        d_model,\\n        n_heads,\\n        num_layers,\\n        max_seq_len,\\n        activation_fn,\\n        seed=42,\\n    ):\\n        super(GPTLikeModel, self).__init__()\\n        # Fix random seed for reproducibility\\n        torch.manual_seed(seed)\\n        if torch.cuda.is_available():\\n            torch.cuda.manual_seed(seed)\\n            torch.cuda.manual_seed_all(seed)\\n\\n        self.embedding = nn.Embedding(vocab_size, d_model)\\n        self.positional_encoding = nn.Parameter(torch.zeros(1, max_seq_len, d_model))\\n\\n        self.transformer_layers = nn.ModuleList(\\n            [\\n                nn.TransformerDecoderLayer(\\n                    d_model=d_model, nhead=n_heads, activation=activation_fn\\n                )\\n                for _ in range(num_layers)\\n            ]\\n        )\\n\\n        self.fc_out = nn.Linear(d_model, vocab_size)\\n\\n    def forward(self, x):\\n        # Embedding and positional encoding\\n        seq_len = x.size(1)\\n        x = self.embedding(x) + self.positional_encoding[:, :seq_len, :]\\n\\n        # Pass through each transformer decoder layer\\n        for layer in self.transformer_layers:\\n            x = layer(x, x)  # Decoder takes input twice in GPT-like models\\n\\n        # Output layer\\n        logits = self.fc_out(x)\\n        return logits\\n\\n\\n# other logic\\n\\n\\n# Define a constant for the filename format\\n# Example function to save results\\ndef save_results(results, filename):\\n    with open(filename, \\\"wb\\\") as f:\\n        pickle.dump(results, f)\\n    print(f\\\"Results saved to {filename}\\\")\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ===============================\n",
    "# For data\n",
    "# ===============================\n",
    "\n",
    "\n",
    "def tokenize_columns(df_in, columns):\n",
    "    df = deepcopy(df_in)\n",
    "    # Create an empty vocabulary\n",
    "    vocab = {}\n",
    "    token_counter = (\n",
    "        1  # Start token IDs from 1 (you can reserve 0 for padding if needed)\n",
    "    )\n",
    "\n",
    "    # Function to add unique column values to the vocab\n",
    "    def add_to_vocab(value):\n",
    "        nonlocal token_counter\n",
    "        if value not in vocab:\n",
    "            vocab[value] = token_counter\n",
    "            token_counter += 1\n",
    "\n",
    "    # Add all unique values from the specified columns to the vocabulary\n",
    "    for column in columns:\n",
    "        df[column].apply(add_to_vocab)\n",
    "\n",
    "    # Function to tokenize a column value based on the vocab\n",
    "    def tokenize(value):\n",
    "        return [\n",
    "            vocab[value]\n",
    "        ]  # Return token ID as a list to keep compatibility with batch processing\n",
    "\n",
    "    # Tokenize the specified columns\n",
    "    for column in columns:\n",
    "        df[f'tokenized_{column.lower().replace(\" \", \"_\")}'] = df[column].apply(tokenize)\n",
    "\n",
    "    # Combine tokenized concept and property into a single input sequence\n",
    "    df[\"input_sequence\"] = df.apply(\n",
    "        lambda row: row[\"tokenized_concept\"] + row[\"tokenized_property\"], axis=1\n",
    "    )\n",
    "\n",
    "    return df, vocab\n",
    "\n",
    "\n",
    "class TripletDataset(Dataset):\n",
    "    def __init__(self, df):\n",
    "        self.inputs = df[\"input_sequence\"].tolist()\n",
    "        self.targets = df[\"tokenized_related_concept\"].tolist()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.inputs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        input_sequence = torch.tensor(self.inputs[idx], dtype=torch.long)\n",
    "        target_sequence = torch.tensor(self.targets[idx], dtype=torch.long)\n",
    "        return input_sequence, target_sequence\n",
    "\n",
    "\n",
    "# ===============================\n",
    "# Model\n",
    "# ===============================\n",
    "\n",
    "\n",
    "# Define the custom activation functions\n",
    "def get_activation_function(name):\n",
    "    if name == \"default\":\n",
    "        return nn.ReLU()\n",
    "    elif name == \"GELU\":\n",
    "        return nn.GELU()\n",
    "    elif name == \"RAF\":\n",
    "        return nn.RReLU()\n",
    "    elif name == \"softmax\":\n",
    "        return nn.Softmax(dim=-1)\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown activation function: {name}\")\n",
    "\n",
    "\n",
    "# Modified model class to include dynamic activation function and adaptable hidden size\n",
    "class GPTLikeModel(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        vocab_size,\n",
    "        d_model,\n",
    "        n_heads,\n",
    "        num_layers,\n",
    "        max_seq_len,\n",
    "        activation_fn,\n",
    "        seed=42,\n",
    "    ):\n",
    "        super(GPTLikeModel, self).__init__()\n",
    "        # Fix random seed for reproducibility\n",
    "        torch.manual_seed(seed)\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.manual_seed(seed)\n",
    "            torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
    "        self.positional_encoding = nn.Parameter(torch.zeros(1, max_seq_len, d_model))\n",
    "\n",
    "        self.transformer_layers = nn.ModuleList(\n",
    "            [\n",
    "                nn.TransformerDecoderLayer(\n",
    "                    d_model=d_model, nhead=n_heads, activation=activation_fn\n",
    "                )\n",
    "                for _ in range(num_layers)\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        self.fc_out = nn.Linear(d_model, vocab_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Embedding and positional encoding\n",
    "        seq_len = x.size(1)\n",
    "        x = self.embedding(x) + self.positional_encoding[:, :seq_len, :]\n",
    "\n",
    "        # Pass through each transformer decoder layer\n",
    "        for layer in self.transformer_layers:\n",
    "            x = layer(x, x)  # Decoder takes input twice in GPT-like models\n",
    "\n",
    "        # Output layer\n",
    "        logits = self.fc_out(x)\n",
    "        return logits\n",
    "\n",
    "\n",
    "# other logic\n",
    "\n",
    "\n",
    "# Define a constant for the filename format\n",
    "# Example function to save results\n",
    "def save_results(results, filename):\n",
    "    with open(filename, \"wb\") as f:\n",
    "        pickle.dump(results, f)\n",
    "    print(f\"Results saved to {filename}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45cbbcbc",
   "metadata": {},
   "source": [
    "## Read and tokenize dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "43aeb321",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-27T20:40:36.260737Z",
     "iopub.status.busy": "2024-10-27T20:40:36.260445Z",
     "iopub.status.idle": "2024-10-27T20:40:39.780506Z",
     "shell.execute_reply": "2024-10-27T20:40:39.779778Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 13;\n",
       "                var nbb_unformatted_code = \"# Load TSV data\\ndata = pd.read_csv(DATA, sep=\\\"\\\\t\\\")\\ndf = data.sample(n=200000, random_state=SEED)\\ncolumns_to_tokenize = [\\\"Concept\\\", \\\"Property\\\", \\\"Related Concept\\\"]\\ndf_tokenized, vocab = tokenize_columns(df, columns_to_tokenize)\";\n",
       "                var nbb_formatted_code = \"# Load TSV data\\ndata = pd.read_csv(DATA, sep=\\\"\\\\t\\\")\\ndf = data.sample(n=200000, random_state=SEED)\\ncolumns_to_tokenize = [\\\"Concept\\\", \\\"Property\\\", \\\"Related Concept\\\"]\\ndf_tokenized, vocab = tokenize_columns(df, columns_to_tokenize)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load TSV data\n",
    "data = pd.read_csv(DATA, sep=\"\\t\")\n",
    "df = data.sample(n=200000, random_state=SEED)\n",
    "columns_to_tokenize = [\"Concept\", \"Property\", \"Related Concept\"]\n",
    "df_tokenized, vocab = tokenize_columns(df, columns_to_tokenize)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec79ec6d",
   "metadata": {},
   "source": [
    "## Train and save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a810cd9e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 15;\n",
       "                var nbb_unformatted_code = \"# Experiment parameters\\nn_values = [50000, 100000]\\nactivation_functions = [\\\"default\\\", \\\"GELU\\\", \\\"RAF\\\", \\\"softmax\\\"]\\nn_layers_values = [1, 2, 4]\\n\\n# Model hyperparameters\\nvocab_size = len(vocab) + 1  # Include 1 for padding (if needed)\\nd_model = 128  # Embedding size\\nn_heads = 4  # Number of attention heads\\nmax_seq_len = 2  # Maximum sequence length (concept + property)\\nbatch_size = 128\\nlr = 0.001\\nepochs = 5\\n\\n# Calculate the number of parameters to keep the total number constant\\nbase_num_layers = 1\\nbase_d_model = d_model\\nbase_num_params = base_d_model * base_num_layers\\n\\n# Prepare the list of all possible configurations using Cartesian product\\nconfigurations = list(\\n    itertools.product(n_values, activation_functions, n_layers_values)\\n)\";\n",
       "                var nbb_formatted_code = \"# Experiment parameters\\nn_values = [50000, 100000]\\nactivation_functions = [\\\"default\\\", \\\"GELU\\\", \\\"RAF\\\", \\\"softmax\\\"]\\nn_layers_values = [1, 2, 4]\\n\\n# Model hyperparameters\\nvocab_size = len(vocab) + 1  # Include 1 for padding (if needed)\\nd_model = 128  # Embedding size\\nn_heads = 4  # Number of attention heads\\nmax_seq_len = 2  # Maximum sequence length (concept + property)\\nbatch_size = 128\\nlr = 0.001\\nepochs = 5\\n\\n# Calculate the number of parameters to keep the total number constant\\nbase_num_layers = 1\\nbase_d_model = d_model\\nbase_num_params = base_d_model * base_num_layers\\n\\n# Prepare the list of all possible configurations using Cartesian product\\nconfigurations = list(\\n    itertools.product(n_values, activation_functions, n_layers_values)\\n)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Experiment parameters\n",
    "n_values = [50000, 100000]\n",
    "activation_functions = [\"default\", \"GELU\", \"RAF\", \"softmax\"]\n",
    "n_layers_values = [1, 2, 4]\n",
    "\n",
    "# Model hyperparameters\n",
    "vocab_size = len(vocab) + 1  # Include 1 for padding (if needed)\n",
    "d_model = 128  # Embedding size\n",
    "n_heads = 4  # Number of attention heads\n",
    "max_seq_len = 2  # Maximum sequence length (concept + property)\n",
    "batch_size = 128\n",
    "lr = 0.001\n",
    "epochs = 5\n",
    "\n",
    "# Calculate the number of parameters to keep the total number constant\n",
    "base_num_layers = 1\n",
    "base_d_model = d_model\n",
    "base_num_params = base_d_model * base_num_layers\n",
    "\n",
    "# Prepare the list of all possible configurations using Cartesian product\n",
    "configurations = list(\n",
    "    itertools.product(n_values, activation_functions, n_layers_values)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "18648aac",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-27T20:40:41.710872Z",
     "iopub.status.busy": "2024-10-27T20:40:41.710589Z",
     "iopub.status.idle": "2024-10-27T21:33:57.529457Z",
     "shell.execute_reply": "2024-10-27T21:33:57.528734Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configurations saved to ../../../data/configs/experiment_configs.pkl\n",
      "24\n",
      "Training with n=50000, activation=default, layers=1, iteration=1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 391/391 [00:08<00:00, 46.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Training Loss: 8.201990501959916\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 391/391 [00:08<00:00, 48.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2, Training Loss: 6.982691484339097\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████| 391/391 [00:01<00:00, 274.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2, Memorization Accuracy: 10.00000%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 391/391 [00:08<00:00, 48.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3, Training Loss: 6.752909404237557\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 391/391 [00:08<00:00, 48.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4, Training Loss: 6.193129549246005\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████| 391/391 [00:01<00:00, 272.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4, Memorization Accuracy: 13.14400%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 391/391 [00:08<00:00, 48.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5, Training Loss: 4.832230626469683\n",
      "Results saved to ../../../data/out_metrics/results_20241118_221136_lay_act_0_2.pkl\n",
      "Results saved to ../../../data/out_models/models_20241118_221136_lay_act_0_2.pkl\n",
      "Training with n=50000, activation=default, layers=2, iteration=1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 391/391 [00:06<00:00, 58.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Training Loss: 8.292995662640427\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 391/391 [00:06<00:00, 58.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2, Training Loss: 6.964269340495624\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████| 391/391 [00:01<00:00, 256.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2, Memorization Accuracy: 9.93800%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 391/391 [00:06<00:00, 58.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3, Training Loss: 6.798611595807478\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 391/391 [00:06<00:00, 58.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4, Training Loss: 6.653622833359272\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████| 391/391 [00:01<00:00, 256.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4, Memorization Accuracy: 12.16200%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 391/391 [00:06<00:00, 58.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5, Training Loss: 6.263756831283764\n",
      "Results saved to ../../../data/out_metrics/results_20241118_221213_lay_act_0_2.pkl\n",
      "Results saved to ../../../data/out_models/models_20241118_221213_lay_act_0_2.pkl\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 4;\n",
       "                var nbb_unformatted_code = \"# Experiment parameters\\nn_values = [50000, 100000]\\nactivation_functions = [\\\"default\\\", \\\"GELU\\\", \\\"RAF\\\", \\\"softmax\\\"]\\nn_layers_values = [1, 2, 4]\\n\\n# Model hyperparameters\\nvocab_size = len(vocab) + 1  # Include 1 for padding (if needed)\\nd_model = 128  # Embedding size\\nn_heads = 4  # Number of attention heads\\nmax_seq_len = 2  # Maximum sequence length (concept + property)\\nbatch_size = 128\\nlr = 0.001\\nepochs = 5\\n\\n# Calculate the number of parameters to keep the total number constant\\nbase_num_layers = 1\\nbase_d_model = d_model\\nbase_num_params = base_d_model * base_num_layers\\n\\n# Prepare the list of all possible configurations using Cartesian product\\nconfigurations = list(\\n    itertools.product(n_values, activation_functions, n_layers_values)\\n)\\n\\n# Save configurations to a file\\nconfig_filename = \\\"../../../data/configs/experiment_configs.pkl\\\"\\nos.makedirs(os.path.dirname(config_filename), exist_ok=True)\\nwith open(config_filename, \\\"wb\\\") as f:\\n    pickle.dump(configurations, f)\\n    print(f\\\"Configurations saved to {config_filename}\\\")\\n\\nprint(len(configurations))\\n\\n# Load configurations and determine starting point\\nstart_index, end_index = int(0), int(2)\\n\\nresults = defaultdict(list)\\nfinal_models = {}\\nnum_iterations = 1\\n\\n# Iterate through configurations starting from the specified index\\nfor config_index in range(start_index, end_index):\\n    n, activation_fn_name, n_layers = configurations[config_index]\\n    adjusted_d_model = int(base_num_params / n_layers)\\n    activation_fn = get_activation_function(activation_fn_name)\\n\\n    for iteration in range(num_iterations):\\n        results_for_n = []  # Create a separate list for each n value\\n        print(f\\\"Training with n={n}, activation={activation_fn_name}, layers={n_layers}, iteration={iteration + 1}\\\")\\n        iteration_seed = SEED + iteration\\n\\n        # Prepare dataset and data loader\\n        dataset = TripletDataset(df_tokenized.sample(n=n, random_state=iteration_seed))\\n        train_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\\n\\n        # Initialize the model with adjusted d_model and activation function\\n        model = GPTLikeModel(\\n            vocab_size, adjusted_d_model, n_heads, n_layers, max_seq_len, activation_fn, seed=iteration_seed\\n        )\\n\\n        # Move the model to GPU if available\\n        device = torch.device(\\\"cuda:3\\\" if torch.cuda.is_available() else \\\"cpu\\\")\\n        model = model.to(device)\\n\\n        # Define the optimizer and loss function\\n        optimizer = optim.Adam(model.parameters(), lr=lr)\\n        criterion = nn.CrossEntropyLoss()\\n\\n        # RUN LAUNCHING\\n        model.train()\\n\\n        for epoch in range(epochs):\\n            total_loss = 0\\n            model.train()  # Set model to training mode\\n\\n            # Training on the same data\\n            for batch in tqdm(train_loader):\\n                inputs, targets = batch\\n                inputs = inputs.to(device)\\n                targets = targets.to(device)\\n\\n                # Forward pass\\n                optimizer.zero_grad()  # can be placed anywhere before loss.backward\\n                outputs = model(inputs)\\n\\n                # We only care about the first token in the output sequence\\n                outputs = outputs[:, 0, :]  # Shape becomes: (batch_size, vocab_size)\\n\\n                targets = targets.view(-1)  # Flatten the targets\\n\\n                # Compute loss\\n                loss = criterion(outputs, targets)\\n\\n                # Backward pass and optimization\\n                loss.backward()\\n                optimizer.step()\\n\\n                total_loss += loss.item()\\n\\n            print(f\\\"Epoch {epoch + 1}, Training Loss: {total_loss / len(train_loader)}\\\")\\n            if epoch % 2 == 0:\\n                continue # skip 0,2,4... (e.g. if n_iters=100, so we plot 1,3,..99 (99th is the last))\\n            # Testing on the same data (memorization check)\\n            model.eval()  # Set model to evaluation mode\\n            correct = 0\\n            total = 0\\n            with torch.no_grad():\\n                for batch in tqdm(train_loader):  # Testing on the same dataset\\n                    inputs, targets = batch\\n                    inputs = inputs.to(device)\\n                    targets = targets.to(device)\\n\\n                    outputs = model(inputs)\\n                    outputs = outputs[:, 0, :]  # Only take the first token prediction\\n                    predicted = torch.argmax(outputs, dim=1)\\n\\n                    total += targets.size(0)\\n                    correct += (predicted == targets.view(-1)).sum().item()\\n            #             print(total, correct)\\n\\n            accuracy = 100 * correct / total\\n            print(f\\\"Epoch {epoch + 1}, Memorization Accuracy: {accuracy:.5f}%\\\")\\n            results_for_n.append(accuracy)\\n        # Save the final model for this iteration\\n        final_models[(n, activation_fn_name, n_layers, iteration)] = model\\n        # Save all accuracies for the current n value\\n        results[(n, activation_fn_name, n_layers)].append(results_for_n)\\n\\n    # Add timestamp to filename\\n    timestamp = datetime.now().strftime(\\\"%Y%m%d_%H%M%S\\\")\\n    indexes = f\\\"{start_index}_{end_index}\\\"\\n    # Pickle the results and final models dictionaries\\n    save_results(results, RESULTS_FILE.format(**{'timestamp': timestamp, 'config_index': indexes}))\\n    save_results(final_models, MODELS_FILE.format(**{'timestamp': timestamp, 'config_index': indexes}))\";\n",
       "                var nbb_formatted_code = \"# Experiment parameters\\nn_values = [50000, 100000]\\nactivation_functions = [\\\"default\\\", \\\"GELU\\\", \\\"RAF\\\", \\\"softmax\\\"]\\nn_layers_values = [1, 2, 4]\\n\\n# Model hyperparameters\\nvocab_size = len(vocab) + 1  # Include 1 for padding (if needed)\\nd_model = 128  # Embedding size\\nn_heads = 4  # Number of attention heads\\nmax_seq_len = 2  # Maximum sequence length (concept + property)\\nbatch_size = 128\\nlr = 0.001\\nepochs = 5\\n\\n# Calculate the number of parameters to keep the total number constant\\nbase_num_layers = 1\\nbase_d_model = d_model\\nbase_num_params = base_d_model * base_num_layers\\n\\n# Prepare the list of all possible configurations using Cartesian product\\nconfigurations = list(\\n    itertools.product(n_values, activation_functions, n_layers_values)\\n)\\n\\n# Save configurations to a file\\nconfig_filename = \\\"../../../data/configs/experiment_configs.pkl\\\"\\nos.makedirs(os.path.dirname(config_filename), exist_ok=True)\\nwith open(config_filename, \\\"wb\\\") as f:\\n    pickle.dump(configurations, f)\\n    print(f\\\"Configurations saved to {config_filename}\\\")\\n\\nprint(len(configurations))\\n\\n# Load configurations and determine starting point\\nstart_index, end_index = int(0), int(2)\\n\\nresults = defaultdict(list)\\nfinal_models = {}\\nnum_iterations = 1\\n\\n# Iterate through configurations starting from the specified index\\nfor config_index in range(start_index, end_index):\\n    n, activation_fn_name, n_layers = configurations[config_index]\\n    adjusted_d_model = int(base_num_params / n_layers)\\n    activation_fn = get_activation_function(activation_fn_name)\\n\\n    for iteration in range(num_iterations):\\n        results_for_n = []  # Create a separate list for each n value\\n        print(\\n            f\\\"Training with n={n}, activation={activation_fn_name}, layers={n_layers}, iteration={iteration + 1}\\\"\\n        )\\n        iteration_seed = SEED + iteration\\n\\n        # Prepare dataset and data loader\\n        dataset = TripletDataset(df_tokenized.sample(n=n, random_state=iteration_seed))\\n        train_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\\n\\n        # Initialize the model with adjusted d_model and activation function\\n        model = GPTLikeModel(\\n            vocab_size,\\n            adjusted_d_model,\\n            n_heads,\\n            n_layers,\\n            max_seq_len,\\n            activation_fn,\\n            seed=iteration_seed,\\n        )\\n\\n        # Move the model to GPU if available\\n        device = torch.device(\\\"cuda:3\\\" if torch.cuda.is_available() else \\\"cpu\\\")\\n        model = model.to(device)\\n\\n        # Define the optimizer and loss function\\n        optimizer = optim.Adam(model.parameters(), lr=lr)\\n        criterion = nn.CrossEntropyLoss()\\n\\n        # RUN LAUNCHING\\n        model.train()\\n\\n        for epoch in range(epochs):\\n            total_loss = 0\\n            model.train()  # Set model to training mode\\n\\n            # Training on the same data\\n            for batch in tqdm(train_loader):\\n                inputs, targets = batch\\n                inputs = inputs.to(device)\\n                targets = targets.to(device)\\n\\n                # Forward pass\\n                optimizer.zero_grad()  # can be placed anywhere before loss.backward\\n                outputs = model(inputs)\\n\\n                # We only care about the first token in the output sequence\\n                outputs = outputs[:, 0, :]  # Shape becomes: (batch_size, vocab_size)\\n\\n                targets = targets.view(-1)  # Flatten the targets\\n\\n                # Compute loss\\n                loss = criterion(outputs, targets)\\n\\n                # Backward pass and optimization\\n                loss.backward()\\n                optimizer.step()\\n\\n                total_loss += loss.item()\\n\\n            print(f\\\"Epoch {epoch + 1}, Training Loss: {total_loss / len(train_loader)}\\\")\\n            if epoch % 2 == 0:\\n                continue  # skip 0,2,4... (e.g. if n_iters=100, so we plot 1,3,..99 (99th is the last))\\n            # Testing on the same data (memorization check)\\n            model.eval()  # Set model to evaluation mode\\n            correct = 0\\n            total = 0\\n            with torch.no_grad():\\n                for batch in tqdm(train_loader):  # Testing on the same dataset\\n                    inputs, targets = batch\\n                    inputs = inputs.to(device)\\n                    targets = targets.to(device)\\n\\n                    outputs = model(inputs)\\n                    outputs = outputs[:, 0, :]  # Only take the first token prediction\\n                    predicted = torch.argmax(outputs, dim=1)\\n\\n                    total += targets.size(0)\\n                    correct += (predicted == targets.view(-1)).sum().item()\\n            #             print(total, correct)\\n\\n            accuracy = 100 * correct / total\\n            print(f\\\"Epoch {epoch + 1}, Memorization Accuracy: {accuracy:.5f}%\\\")\\n            results_for_n.append(accuracy)\\n        # Save the final model for this iteration\\n        final_models[(n, activation_fn_name, n_layers, iteration)] = model\\n        # Save all accuracies for the current n value\\n        results[(n, activation_fn_name, n_layers)].append(results_for_n)\\n\\n    # Add timestamp to filename\\n    timestamp = datetime.now().strftime(\\\"%Y%m%d_%H%M%S\\\")\\n    indexes = f\\\"{start_index}_{end_index}\\\"\\n    # Pickle the results and final models dictionaries\\n    save_results(\\n        results,\\n        RESULTS_FILE.format(**{\\\"timestamp\\\": timestamp, \\\"config_index\\\": indexes}),\\n    )\\n    save_results(\\n        final_models,\\n        MODELS_FILE.format(**{\\\"timestamp\\\": timestamp, \\\"config_index\\\": indexes}),\\n    )\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Save configurations to a file\n",
    "config_filename = \"../../../data/configs/experiment_configs.pkl\"\n",
    "os.makedirs(os.path.dirname(config_filename), exist_ok=True)\n",
    "with open(config_filename, \"wb\") as f:\n",
    "    pickle.dump(configurations, f)\n",
    "    print(f\"Configurations saved to {config_filename}\")\n",
    "\n",
    "print(len(configurations))\n",
    "\n",
    "# Load configurations and determine starting point\n",
    "start_index, end_index = int(0), int(2)\n",
    "\n",
    "results = defaultdict(list)\n",
    "final_models = {}\n",
    "num_iterations = 1\n",
    "\n",
    "# Iterate through configurations starting from the specified index\n",
    "for config_index in range(start_index, end_index):\n",
    "    n, activation_fn_name, n_layers = configurations[config_index]\n",
    "    adjusted_d_model = int(base_num_params / n_layers)\n",
    "    activation_fn = get_activation_function(activation_fn_name)\n",
    "\n",
    "    for iteration in range(num_iterations):\n",
    "        results_for_n = []  # Create a separate list for each n value\n",
    "        print(\n",
    "            f\"Training with n={n}, activation={activation_fn_name}, layers={n_layers}, iteration={iteration + 1}\"\n",
    "        )\n",
    "        iteration_seed = SEED + iteration\n",
    "\n",
    "        # Prepare dataset and data loader\n",
    "        dataset = TripletDataset(df_tokenized.sample(n=n, random_state=iteration_seed))\n",
    "        train_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "        # Initialize the model with adjusted d_model and activation function\n",
    "        model = GPTLikeModel(\n",
    "            vocab_size,\n",
    "            adjusted_d_model,\n",
    "            n_heads,\n",
    "            n_layers,\n",
    "            max_seq_len,\n",
    "            activation_fn,\n",
    "            seed=iteration_seed,\n",
    "        )\n",
    "\n",
    "        # Move the model to GPU if available\n",
    "        device = torch.device(\"cuda:3\" if torch.cuda.is_available() else \"cpu\")\n",
    "        model = model.to(device)\n",
    "\n",
    "        # Define the optimizer and loss function\n",
    "        optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "        # RUN LAUNCHING\n",
    "        model.train()\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            total_loss = 0\n",
    "            model.train()  # Set model to training mode\n",
    "\n",
    "            # Training on the same data\n",
    "            for batch in tqdm(train_loader):\n",
    "                inputs, targets = batch\n",
    "                inputs = inputs.to(device)\n",
    "                targets = targets.to(device)\n",
    "\n",
    "                # Forward pass\n",
    "                optimizer.zero_grad()  # can be placed anywhere before loss.backward\n",
    "                outputs = model(inputs)\n",
    "\n",
    "                # We only care about the first token in the output sequence\n",
    "                outputs = outputs[:, 0, :]  # Shape becomes: (batch_size, vocab_size)\n",
    "\n",
    "                targets = targets.view(-1)  # Flatten the targets\n",
    "\n",
    "                # Compute loss\n",
    "                loss = criterion(outputs, targets)\n",
    "\n",
    "                # Backward pass and optimization\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                total_loss += loss.item()\n",
    "\n",
    "            print(f\"Epoch {epoch + 1}, Training Loss: {total_loss / len(train_loader)}\")\n",
    "            if epoch % 2 == 0:\n",
    "                continue  # skip 0,2,4... (e.g. if n_iters=100, so we plot 1,3,..99 (99th is the last))\n",
    "            # Testing on the same data (memorization check)\n",
    "            model.eval()  # Set model to evaluation mode\n",
    "            correct = 0\n",
    "            total = 0\n",
    "            with torch.no_grad():\n",
    "                for batch in tqdm(train_loader):  # Testing on the same dataset\n",
    "                    inputs, targets = batch\n",
    "                    inputs = inputs.to(device)\n",
    "                    targets = targets.to(device)\n",
    "\n",
    "                    outputs = model(inputs)\n",
    "                    outputs = outputs[:, 0, :]  # Only take the first token prediction\n",
    "                    predicted = torch.argmax(outputs, dim=1)\n",
    "\n",
    "                    total += targets.size(0)\n",
    "                    correct += (predicted == targets.view(-1)).sum().item()\n",
    "            #             print(total, correct)\n",
    "\n",
    "            accuracy = 100 * correct / total\n",
    "            print(f\"Epoch {epoch + 1}, Memorization Accuracy: {accuracy:.5f}%\")\n",
    "            results_for_n.append(accuracy)\n",
    "        # Save the final model for this iteration\n",
    "        final_models[(n, activation_fn_name, n_layers, iteration)] = model\n",
    "        # Save all accuracies for the current n value\n",
    "        results[(n, activation_fn_name, n_layers)].append(results_for_n)\n",
    "\n",
    "    # Add timestamp to filename\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    indexes = f\"{start_index}_{end_index}\"\n",
    "    # Pickle the results and final models dictionaries\n",
    "    save_results(\n",
    "        results,\n",
    "        RESULTS_FILE.format(**{\"timestamp\": timestamp, \"config_index\": indexes}),\n",
    "    )\n",
    "    save_results(\n",
    "        final_models,\n",
    "        MODELS_FILE.format(**{\"timestamp\": timestamp, \"config_index\": indexes}),\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "49e5feb4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 25;\n",
       "                var nbb_unformatted_code = \"import pickle\\n\\nwith open(\\n    \\\"../../../data/out_models/models_20241119_083532_lay_act_0_6.pkl\\\", \\\"rb\\\"\\n) as file:\\n    results = dict(pickle.load(file))\";\n",
       "                var nbb_formatted_code = \"import pickle\\n\\nwith open(\\n    \\\"../../../data/out_models/models_20241119_083532_lay_act_0_6.pkl\\\", \\\"rb\\\"\\n) as file:\\n    results = dict(pickle.load(file))\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "with open(\n",
    "    \"../../../data/out_models/models_20241119_083532_lay_act_0_6.pkl\", \"rb\"\n",
    ") as file:\n",
    "    results = dict(pickle.load(file))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8a41bd7c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{(50000,\n",
       "  'default',\n",
       "  1,\n",
       "  0): GPTLikeModel(\n",
       "   (embedding): Embedding(200353, 128)\n",
       "   (transformer_layers): ModuleList(\n",
       "     (0): TransformerDecoderLayer(\n",
       "       (self_attn): MultiheadAttention(\n",
       "         (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
       "       )\n",
       "       (multihead_attn): MultiheadAttention(\n",
       "         (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
       "       )\n",
       "       (linear1): Linear(in_features=128, out_features=2048, bias=True)\n",
       "       (dropout): Dropout(p=0.1, inplace=False)\n",
       "       (linear2): Linear(in_features=2048, out_features=128, bias=True)\n",
       "       (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "       (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "       (norm3): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "       (dropout1): Dropout(p=0.1, inplace=False)\n",
       "       (dropout2): Dropout(p=0.1, inplace=False)\n",
       "       (dropout3): Dropout(p=0.1, inplace=False)\n",
       "       (activation): ReLU()\n",
       "     )\n",
       "   )\n",
       "   (fc_out): Linear(in_features=128, out_features=200353, bias=True)\n",
       " ),\n",
       " (50000,\n",
       "  'default',\n",
       "  1,\n",
       "  1): GPTLikeModel(\n",
       "   (embedding): Embedding(200353, 128)\n",
       "   (transformer_layers): ModuleList(\n",
       "     (0): TransformerDecoderLayer(\n",
       "       (self_attn): MultiheadAttention(\n",
       "         (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
       "       )\n",
       "       (multihead_attn): MultiheadAttention(\n",
       "         (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
       "       )\n",
       "       (linear1): Linear(in_features=128, out_features=2048, bias=True)\n",
       "       (dropout): Dropout(p=0.1, inplace=False)\n",
       "       (linear2): Linear(in_features=2048, out_features=128, bias=True)\n",
       "       (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "       (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "       (norm3): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "       (dropout1): Dropout(p=0.1, inplace=False)\n",
       "       (dropout2): Dropout(p=0.1, inplace=False)\n",
       "       (dropout3): Dropout(p=0.1, inplace=False)\n",
       "       (activation): ReLU()\n",
       "     )\n",
       "   )\n",
       "   (fc_out): Linear(in_features=128, out_features=200353, bias=True)\n",
       " ),\n",
       " (50000,\n",
       "  'default',\n",
       "  1,\n",
       "  2): GPTLikeModel(\n",
       "   (embedding): Embedding(200353, 128)\n",
       "   (transformer_layers): ModuleList(\n",
       "     (0): TransformerDecoderLayer(\n",
       "       (self_attn): MultiheadAttention(\n",
       "         (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
       "       )\n",
       "       (multihead_attn): MultiheadAttention(\n",
       "         (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
       "       )\n",
       "       (linear1): Linear(in_features=128, out_features=2048, bias=True)\n",
       "       (dropout): Dropout(p=0.1, inplace=False)\n",
       "       (linear2): Linear(in_features=2048, out_features=128, bias=True)\n",
       "       (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "       (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "       (norm3): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "       (dropout1): Dropout(p=0.1, inplace=False)\n",
       "       (dropout2): Dropout(p=0.1, inplace=False)\n",
       "       (dropout3): Dropout(p=0.1, inplace=False)\n",
       "       (activation): ReLU()\n",
       "     )\n",
       "   )\n",
       "   (fc_out): Linear(in_features=128, out_features=200353, bias=True)\n",
       " ),\n",
       " (50000,\n",
       "  'default',\n",
       "  1,\n",
       "  3): GPTLikeModel(\n",
       "   (embedding): Embedding(200353, 128)\n",
       "   (transformer_layers): ModuleList(\n",
       "     (0): TransformerDecoderLayer(\n",
       "       (self_attn): MultiheadAttention(\n",
       "         (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
       "       )\n",
       "       (multihead_attn): MultiheadAttention(\n",
       "         (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
       "       )\n",
       "       (linear1): Linear(in_features=128, out_features=2048, bias=True)\n",
       "       (dropout): Dropout(p=0.1, inplace=False)\n",
       "       (linear2): Linear(in_features=2048, out_features=128, bias=True)\n",
       "       (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "       (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "       (norm3): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "       (dropout1): Dropout(p=0.1, inplace=False)\n",
       "       (dropout2): Dropout(p=0.1, inplace=False)\n",
       "       (dropout3): Dropout(p=0.1, inplace=False)\n",
       "       (activation): ReLU()\n",
       "     )\n",
       "   )\n",
       "   (fc_out): Linear(in_features=128, out_features=200353, bias=True)\n",
       " ),\n",
       " (50000,\n",
       "  'default',\n",
       "  1,\n",
       "  4): GPTLikeModel(\n",
       "   (embedding): Embedding(200353, 128)\n",
       "   (transformer_layers): ModuleList(\n",
       "     (0): TransformerDecoderLayer(\n",
       "       (self_attn): MultiheadAttention(\n",
       "         (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
       "       )\n",
       "       (multihead_attn): MultiheadAttention(\n",
       "         (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
       "       )\n",
       "       (linear1): Linear(in_features=128, out_features=2048, bias=True)\n",
       "       (dropout): Dropout(p=0.1, inplace=False)\n",
       "       (linear2): Linear(in_features=2048, out_features=128, bias=True)\n",
       "       (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "       (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "       (norm3): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "       (dropout1): Dropout(p=0.1, inplace=False)\n",
       "       (dropout2): Dropout(p=0.1, inplace=False)\n",
       "       (dropout3): Dropout(p=0.1, inplace=False)\n",
       "       (activation): ReLU()\n",
       "     )\n",
       "   )\n",
       "   (fc_out): Linear(in_features=128, out_features=200353, bias=True)\n",
       " ),\n",
       " (50000,\n",
       "  'default',\n",
       "  1,\n",
       "  5): GPTLikeModel(\n",
       "   (embedding): Embedding(200353, 128)\n",
       "   (transformer_layers): ModuleList(\n",
       "     (0): TransformerDecoderLayer(\n",
       "       (self_attn): MultiheadAttention(\n",
       "         (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
       "       )\n",
       "       (multihead_attn): MultiheadAttention(\n",
       "         (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
       "       )\n",
       "       (linear1): Linear(in_features=128, out_features=2048, bias=True)\n",
       "       (dropout): Dropout(p=0.1, inplace=False)\n",
       "       (linear2): Linear(in_features=2048, out_features=128, bias=True)\n",
       "       (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "       (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "       (norm3): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "       (dropout1): Dropout(p=0.1, inplace=False)\n",
       "       (dropout2): Dropout(p=0.1, inplace=False)\n",
       "       (dropout3): Dropout(p=0.1, inplace=False)\n",
       "       (activation): ReLU()\n",
       "     )\n",
       "   )\n",
       "   (fc_out): Linear(in_features=128, out_features=200353, bias=True)\n",
       " ),\n",
       " (50000,\n",
       "  'default',\n",
       "  1,\n",
       "  6): GPTLikeModel(\n",
       "   (embedding): Embedding(200353, 128)\n",
       "   (transformer_layers): ModuleList(\n",
       "     (0): TransformerDecoderLayer(\n",
       "       (self_attn): MultiheadAttention(\n",
       "         (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
       "       )\n",
       "       (multihead_attn): MultiheadAttention(\n",
       "         (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
       "       )\n",
       "       (linear1): Linear(in_features=128, out_features=2048, bias=True)\n",
       "       (dropout): Dropout(p=0.1, inplace=False)\n",
       "       (linear2): Linear(in_features=2048, out_features=128, bias=True)\n",
       "       (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "       (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "       (norm3): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "       (dropout1): Dropout(p=0.1, inplace=False)\n",
       "       (dropout2): Dropout(p=0.1, inplace=False)\n",
       "       (dropout3): Dropout(p=0.1, inplace=False)\n",
       "       (activation): ReLU()\n",
       "     )\n",
       "   )\n",
       "   (fc_out): Linear(in_features=128, out_features=200353, bias=True)\n",
       " ),\n",
       " (50000,\n",
       "  'default',\n",
       "  1,\n",
       "  7): GPTLikeModel(\n",
       "   (embedding): Embedding(200353, 128)\n",
       "   (transformer_layers): ModuleList(\n",
       "     (0): TransformerDecoderLayer(\n",
       "       (self_attn): MultiheadAttention(\n",
       "         (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
       "       )\n",
       "       (multihead_attn): MultiheadAttention(\n",
       "         (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
       "       )\n",
       "       (linear1): Linear(in_features=128, out_features=2048, bias=True)\n",
       "       (dropout): Dropout(p=0.1, inplace=False)\n",
       "       (linear2): Linear(in_features=2048, out_features=128, bias=True)\n",
       "       (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "       (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "       (norm3): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "       (dropout1): Dropout(p=0.1, inplace=False)\n",
       "       (dropout2): Dropout(p=0.1, inplace=False)\n",
       "       (dropout3): Dropout(p=0.1, inplace=False)\n",
       "       (activation): ReLU()\n",
       "     )\n",
       "   )\n",
       "   (fc_out): Linear(in_features=128, out_features=200353, bias=True)\n",
       " ),\n",
       " (50000,\n",
       "  'default',\n",
       "  1,\n",
       "  8): GPTLikeModel(\n",
       "   (embedding): Embedding(200353, 128)\n",
       "   (transformer_layers): ModuleList(\n",
       "     (0): TransformerDecoderLayer(\n",
       "       (self_attn): MultiheadAttention(\n",
       "         (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
       "       )\n",
       "       (multihead_attn): MultiheadAttention(\n",
       "         (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
       "       )\n",
       "       (linear1): Linear(in_features=128, out_features=2048, bias=True)\n",
       "       (dropout): Dropout(p=0.1, inplace=False)\n",
       "       (linear2): Linear(in_features=2048, out_features=128, bias=True)\n",
       "       (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "       (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "       (norm3): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "       (dropout1): Dropout(p=0.1, inplace=False)\n",
       "       (dropout2): Dropout(p=0.1, inplace=False)\n",
       "       (dropout3): Dropout(p=0.1, inplace=False)\n",
       "       (activation): ReLU()\n",
       "     )\n",
       "   )\n",
       "   (fc_out): Linear(in_features=128, out_features=200353, bias=True)\n",
       " ),\n",
       " (50000,\n",
       "  'default',\n",
       "  1,\n",
       "  9): GPTLikeModel(\n",
       "   (embedding): Embedding(200353, 128)\n",
       "   (transformer_layers): ModuleList(\n",
       "     (0): TransformerDecoderLayer(\n",
       "       (self_attn): MultiheadAttention(\n",
       "         (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
       "       )\n",
       "       (multihead_attn): MultiheadAttention(\n",
       "         (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
       "       )\n",
       "       (linear1): Linear(in_features=128, out_features=2048, bias=True)\n",
       "       (dropout): Dropout(p=0.1, inplace=False)\n",
       "       (linear2): Linear(in_features=2048, out_features=128, bias=True)\n",
       "       (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "       (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "       (norm3): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "       (dropout1): Dropout(p=0.1, inplace=False)\n",
       "       (dropout2): Dropout(p=0.1, inplace=False)\n",
       "       (dropout3): Dropout(p=0.1, inplace=False)\n",
       "       (activation): ReLU()\n",
       "     )\n",
       "   )\n",
       "   (fc_out): Linear(in_features=128, out_features=200353, bias=True)\n",
       " )}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 26;\n",
       "                var nbb_unformatted_code = \"results\";\n",
       "                var nbb_formatted_code = \"results\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8a318dd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configurations saved to ../../../data/configs/experiment_configs.pkl\n",
      "24\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 16;\n",
       "                var nbb_unformatted_code = \"# Save configurations to a file\\nconfig_filename = \\\"../../../data/configs/experiment_configs.pkl\\\"\\nos.makedirs(os.path.dirname(config_filename), exist_ok=True)\\nwith open(config_filename, \\\"wb\\\") as f:\\n    pickle.dump(configurations, f)\\n    print(f\\\"Configurations saved to {config_filename}\\\")\\n\\nprint(len(configurations))\\n\\n# Load configurations and determine starting point\\nstart_index, end_index = int(0), int(2)\\n\\nresults = defaultdict(list)\\nfinal_models = {}\\nnum_iterations = 1\\n\\n# Iterate through configurations starting from the specified index\\nfor config_index in range(start_index, end_index):\\n    n, activation_fn_name, n_layers = configurations[config_index]\\n    adjusted_d_model = int(base_num_params / n_layers)\\n    activation_fn = get_activation_function(activation_fn_name)\\n\\n    for iteration in range(num_iterations):\\n        break\";\n",
       "                var nbb_formatted_code = \"# Save configurations to a file\\nconfig_filename = \\\"../../../data/configs/experiment_configs.pkl\\\"\\nos.makedirs(os.path.dirname(config_filename), exist_ok=True)\\nwith open(config_filename, \\\"wb\\\") as f:\\n    pickle.dump(configurations, f)\\n    print(f\\\"Configurations saved to {config_filename}\\\")\\n\\nprint(len(configurations))\\n\\n# Load configurations and determine starting point\\nstart_index, end_index = int(0), int(2)\\n\\nresults = defaultdict(list)\\nfinal_models = {}\\nnum_iterations = 1\\n\\n# Iterate through configurations starting from the specified index\\nfor config_index in range(start_index, end_index):\\n    n, activation_fn_name, n_layers = configurations[config_index]\\n    adjusted_d_model = int(base_num_params / n_layers)\\n    activation_fn = get_activation_function(activation_fn_name)\\n\\n    for iteration in range(num_iterations):\\n        break\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Save configurations to a file\n",
    "config_filename = \"../../../data/configs/experiment_configs.pkl\"\n",
    "os.makedirs(os.path.dirname(config_filename), exist_ok=True)\n",
    "with open(config_filename, \"wb\") as f:\n",
    "    pickle.dump(configurations, f)\n",
    "    print(f\"Configurations saved to {config_filename}\")\n",
    "\n",
    "print(len(configurations))\n",
    "\n",
    "# Load configurations and determine starting point\n",
    "start_index, end_index = int(0), int(2)\n",
    "\n",
    "results = defaultdict(list)\n",
    "final_models = {}\n",
    "num_iterations = 1\n",
    "\n",
    "# Iterate through configurations starting from the specified index\n",
    "for config_index in range(start_index, end_index):\n",
    "    n, activation_fn_name, n_layers = configurations[config_index]\n",
    "    adjusted_d_model = int(base_num_params / n_layers)\n",
    "    activation_fn = get_activation_function(activation_fn_name)\n",
    "\n",
    "    for iteration in range(num_iterations):\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "a3a3bee1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPTLikeModel(\n",
       "  (embedding): Embedding(200353, 128)\n",
       "  (transformer_layers): ModuleList(\n",
       "    (0): TransformerDecoderLayer(\n",
       "      (self_attn): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
       "      )\n",
       "      (multihead_attn): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
       "      )\n",
       "      (linear1): Linear(in_features=128, out_features=2048, bias=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "      (linear2): Linear(in_features=2048, out_features=128, bias=True)\n",
       "      (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "      (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "      (norm3): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout1): Dropout(p=0.1, inplace=False)\n",
       "      (dropout2): Dropout(p=0.1, inplace=False)\n",
       "      (dropout3): Dropout(p=0.1, inplace=False)\n",
       "      (activation): ReLU()\n",
       "    )\n",
       "  )\n",
       "  (fc_out): Linear(in_features=128, out_features=200353, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 34;\n",
       "                var nbb_unformatted_code = \"model = results[(50000, \\\"default\\\", 1, 1)]\\ndevice = torch.device(\\\"cuda:3\\\" if torch.cuda.is_available() else \\\"cpu\\\")\\nmodel = model.to(device)\\nmodel\";\n",
       "                var nbb_formatted_code = \"model = results[(50000, \\\"default\\\", 1, 1)]\\ndevice = torch.device(\\\"cuda:3\\\" if torch.cuda.is_available() else \\\"cpu\\\")\\nmodel = model.to(device)\\nmodel\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = results[(50000, \"default\", 1, 1)]\n",
    "device = torch.device(\"cuda:3\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "73d8a81d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████| 782/782 [00:02<00:00, 368.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2, Memorization Accuracy: 0.0%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 36;\n",
       "                var nbb_unformatted_code = \"# Testing on the same data (memorization check)\\nmodel.eval()  # Set model to evaluation mode\\ncorrect = 0\\ntotal = 0\\nwith torch.no_grad():\\n    for batch in tqdm(train_loader):  # Testing on the same dataset\\n        inputs, targets = batch\\n        inputs = inputs.to(device)\\n        targets = targets.to(device)\\n\\n        outputs = model(inputs)\\n        outputs = outputs[:, 0, :]  # Only take the first token prediction\\n        predicted = torch.argmax(outputs, dim=1)\\n\\n        total += targets.size(0)\\n        correct += (predicted == targets.view(-1)).sum().item()\\n#             print(total, correct)\\n\\naccuracy = 100 * correct / total\\nprint(f\\\"Epoch {epoch + 1}, Memorization Accuracy: {accuracy}%\\\")\";\n",
       "                var nbb_formatted_code = \"# Testing on the same data (memorization check)\\nmodel.eval()  # Set model to evaluation mode\\ncorrect = 0\\ntotal = 0\\nwith torch.no_grad():\\n    for batch in tqdm(train_loader):  # Testing on the same dataset\\n        inputs, targets = batch\\n        inputs = inputs.to(device)\\n        targets = targets.to(device)\\n\\n        outputs = model(inputs)\\n        outputs = outputs[:, 0, :]  # Only take the first token prediction\\n        predicted = torch.argmax(outputs, dim=1)\\n\\n        total += targets.size(0)\\n        correct += (predicted == targets.view(-1)).sum().item()\\n#             print(total, correct)\\n\\naccuracy = 100 * correct / total\\nprint(f\\\"Epoch {epoch + 1}, Memorization Accuracy: {accuracy}%\\\")\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Testing on the same data (memorization check)\n",
    "model.eval()  # Set model to evaluation mode\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for batch in tqdm(train_loader):  # Testing on the same dataset\n",
    "        inputs, targets = batch\n",
    "        inputs = inputs.to(device)\n",
    "        targets = targets.to(device)\n",
    "\n",
    "        outputs = model(inputs)\n",
    "        outputs = outputs[:, 0, :]  # Only take the first token prediction\n",
    "        predicted = torch.argmax(outputs, dim=1)\n",
    "\n",
    "        total += targets.size(0)\n",
    "        correct += (predicted == targets.view(-1)).sum().item()\n",
    "#             print(total, correct)\n",
    "\n",
    "accuracy = 100 * correct / total\n",
    "print(f\"Epoch {epoch + 1}, Memorization Accuracy: {accuracy}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "4a87f5ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 29;\n",
       "                var nbb_unformatted_code = \"device = torch.device(\\\"cuda:3\\\" if torch.cuda.is_available() else \\\"cpu\\\")\\nmodel = model.to(device)\";\n",
       "                var nbb_formatted_code = \"device = torch.device(\\\"cuda:3\\\" if torch.cuda.is_available() else \\\"cpu\\\")\\nmodel = model.to(device)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7a689ad",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

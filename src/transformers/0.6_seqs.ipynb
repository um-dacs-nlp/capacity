{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "43ffa757",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "# coding: utf-8\n",
    "\n",
    "# # Imports, constants\n",
    "\n",
    "# In[1]:\n",
    "# imports\n",
    "import pickle\n",
    "import random\n",
    "import argparse\n",
    "from copy import deepcopy\n",
    "import itertools\n",
    "import os\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from collections import defaultdict\n",
    "from datetime import datetime\n",
    "from pprint import pprint\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "# constants\n",
    "DATA = \"../../../data/created_data/seqs100000.tsv\"\n",
    "SEED = 566\n",
    "# Results and models file paths\n",
    "RESULTS_FILE = (\n",
    "    \"../../../data/out_metrics/results_{timestamp}_seq_{config_index}.pkl\"\n",
    ")\n",
    "LOSSES_FILE = (\n",
    "    \"../../../data/out_metrics/losses_{timestamp}_seq_{config_index}.pkl\"\n",
    ")\n",
    "MODELS_FILE = \"../../../data/out_models/models_{timestamp}_seq_{config_index}.pkl\"\n",
    "\n",
    "# fix random seeds for reproducibility\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(SEED)\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "acb168c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_and_tokenize(file_path):\n",
    "    # Step 1: Read the TSV file with variable columns\n",
    "    data = []\n",
    "    with open(file_path, 'r') as f:\n",
    "        for line in f:\n",
    "            row = line.strip().split('\\t')\n",
    "            data.append(row)\n",
    "\n",
    "    # Determine the maximum number of columns dynamically\n",
    "    max_cols = max(len(row) for row in data)\n",
    "    column_names = [f\"col_{i}\" for i in range(max_cols)]\n",
    "    df = pd.DataFrame(data, columns=column_names)\n",
    "\n",
    "    # Step 2: Replace '.' and ',' with spaces, and handle missing columns by filling with \"0\"\n",
    "    df = df.applymap(lambda x: x.replace('.', ' ').replace(',', ' ') if pd.notnull(x) else \"0\")\n",
    "\n",
    "    # Step 3: Tokenize nodes/edges uniquely\n",
    "    EOS_TOKEN = \"<EOS>\"\n",
    "    node_edge_vocab = {EOS_TOKEN: 1}  # Start with EOS token\n",
    "    node_edge_counter = 2  # Start token IDs from 2 to reserve 1 for EOS\n",
    "\n",
    "    def tokenize_node_edge(value):\n",
    "        nonlocal node_edge_counter\n",
    "        if value not in node_edge_vocab:\n",
    "            node_edge_vocab[value] = node_edge_counter\n",
    "            node_edge_counter += 1\n",
    "        return node_edge_vocab[value]\n",
    "\n",
    "    df_tokenized_node_edge = df.applymap(lambda x: tokenize_node_edge(x) if x != \"0\" else 0)\n",
    "    df_tokenized_node_edge[f\"col_{len(column_names)}\"]=0\n",
    "\n",
    "    # Step 4: Tokenize word-by-word (using \"_\" and spaces as separators)\n",
    "    word_vocab = {EOS_TOKEN: 1}  # Start with EOS token\n",
    "    word_counter = 2  # Start token IDs from 2 to reserve 1 for EOS\n",
    "\n",
    "    def tokenize_word_by_word(value):\n",
    "        nonlocal word_counter\n",
    "        tokens = []\n",
    "        if value != \"0\":\n",
    "            for word in value.replace(\"_\", \" \").replace(\".\", \" \").split():  # Split on spaces and \"_\"\n",
    "                if word not in word_vocab:\n",
    "                    word_vocab[word] = word_counter\n",
    "                    word_counter += 1\n",
    "                tokens.append(word_vocab[word])\n",
    "            tokens.append(word_vocab[EOS_TOKEN])  # Append EOS token\n",
    "        tokens.append(0)  # Append EOS token\n",
    "        return tokens\n",
    "\n",
    "    df_tokenized_word_by_word = df.applymap(lambda x: tokenize_word_by_word(x) if x != \"0\" else [0])\n",
    "\n",
    "    # Step 5: Unflatten word-by-word into a new DataFrame\n",
    "    word_by_word_expanded = []\n",
    "    for index, row in df_tokenized_word_by_word.iterrows():\n",
    "        expanded_row = []\n",
    "        for cell in row:\n",
    "            if isinstance(cell, list):\n",
    "                expanded_row.extend(cell)\n",
    "            else:\n",
    "                expanded_row.append(cell)\n",
    "        word_by_word_expanded.append(expanded_row)\n",
    "\n",
    "    max_words = max(len(row) for row in word_by_word_expanded)\n",
    "    df_word_by_word_unflattened = pd.DataFrame(word_by_word_expanded, columns=[f\"word_{i}\" for i in range(max_words)]).fillna(0).astype(int)\n",
    "\n",
    "    return df, df_tokenized_node_edge, df_word_by_word_unflattened, node_edge_vocab, word_vocab\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "f5177440",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original DataFrame:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>col_0</th>\n",
       "      <th>col_1</th>\n",
       "      <th>col_2</th>\n",
       "      <th>col_3</th>\n",
       "      <th>col_4</th>\n",
       "      <th>col_5</th>\n",
       "      <th>col_6</th>\n",
       "      <th>col_7</th>\n",
       "      <th>col_8</th>\n",
       "      <th>col_9</th>\n",
       "      <th>col_10</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Adolescent onset conduct-dissocial disorder</td>\n",
       "      <td>occurs_in</td>\n",
       "      <td>Adolescence</td>\n",
       "      <td>reversed_occurs_in</td>\n",
       "      <td>groups 3871486_1</td>\n",
       "      <td>occurs_in</td>\n",
       "      <td>Adolescence</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>groups 3530725_0</td>\n",
       "      <td>mapped_to</td>\n",
       "      <td>Neuropathic heredofamilial amyloidosis</td>\n",
       "      <td>reversed_mapped_to</td>\n",
       "      <td>groups 3731020_0</td>\n",
       "      <td>mapped_to</td>\n",
       "      <td>Neuropathic heredofamilial amyloidosis</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Entire superior labial artery</td>\n",
       "      <td>reversed_has_entire_anatomy_structure</td>\n",
       "      <td>groups 3462550_0</td>\n",
       "      <td>has_laterality</td>\n",
       "      <td>Side</td>\n",
       "      <td>reversed_has_laterality</td>\n",
       "      <td>groups 3464901_0</td>\n",
       "      <td>has_laterality</td>\n",
       "      <td>Side</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Phenobarbital 30 mg oral tablet</td>\n",
       "      <td>inactivation_indicator</td>\n",
       "      <td>723277005</td>\n",
       "      <td>reversed_inactivation_indicator</td>\n",
       "      <td>Thromboembolus of vein following surgical proc...</td>\n",
       "      <td>occurs_after</td>\n",
       "      <td>Surgical procedure</td>\n",
       "      <td>reversed_occurs_after</td>\n",
       "      <td>groups 3903443_3</td>\n",
       "      <td>occurs_after</td>\n",
       "      <td>Surgical procedure</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Entire lower plate of the cochlear spiral lamina</td>\n",
       "      <td>has_laterality</td>\n",
       "      <td>Side</td>\n",
       "      <td>laterality_of</td>\n",
       "      <td>Structure of inferior sagittal sinus</td>\n",
       "      <td>reversed_laterality_of</td>\n",
       "      <td>Side</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              col_0  \\\n",
       "0       Adolescent onset conduct-dissocial disorder   \n",
       "1                                  groups 3530725_0   \n",
       "2                     Entire superior labial artery   \n",
       "3                   Phenobarbital 30 mg oral tablet   \n",
       "4  Entire lower plate of the cochlear spiral lamina   \n",
       "\n",
       "                                   col_1  \\\n",
       "0                              occurs_in   \n",
       "1                              mapped_to   \n",
       "2  reversed_has_entire_anatomy_structure   \n",
       "3                 inactivation_indicator   \n",
       "4                         has_laterality   \n",
       "\n",
       "                                    col_2                            col_3  \\\n",
       "0                             Adolescence               reversed_occurs_in   \n",
       "1  Neuropathic heredofamilial amyloidosis               reversed_mapped_to   \n",
       "2                        groups 3462550_0                   has_laterality   \n",
       "3                               723277005  reversed_inactivation_indicator   \n",
       "4                                    Side                    laterality_of   \n",
       "\n",
       "                                               col_4                    col_5  \\\n",
       "0                                   groups 3871486_1                occurs_in   \n",
       "1                                   groups 3731020_0                mapped_to   \n",
       "2                                               Side  reversed_has_laterality   \n",
       "3  Thromboembolus of vein following surgical proc...             occurs_after   \n",
       "4               Structure of inferior sagittal sinus   reversed_laterality_of   \n",
       "\n",
       "                                    col_6                  col_7  \\\n",
       "0                             Adolescence                      0   \n",
       "1  Neuropathic heredofamilial amyloidosis                      0   \n",
       "2                        groups 3464901_0         has_laterality   \n",
       "3                      Surgical procedure  reversed_occurs_after   \n",
       "4                                    Side                      0   \n",
       "\n",
       "              col_8         col_9              col_10  \n",
       "0                 0             0                   0  \n",
       "1                 0             0                   0  \n",
       "2              Side             0                   0  \n",
       "3  groups 3903443_3  occurs_after  Surgical procedure  \n",
       "4                 0             0                   0  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Node/Edge Tokenized DataFrame:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>col_0</th>\n",
       "      <th>col_1</th>\n",
       "      <th>col_2</th>\n",
       "      <th>col_3</th>\n",
       "      <th>col_4</th>\n",
       "      <th>col_5</th>\n",
       "      <th>col_6</th>\n",
       "      <th>col_7</th>\n",
       "      <th>col_8</th>\n",
       "      <th>col_9</th>\n",
       "      <th>col_10</th>\n",
       "      <th>col_11</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>3303</td>\n",
       "      <td>3482</td>\n",
       "      <td>5292</td>\n",
       "      <td>5397</td>\n",
       "      <td>3303</td>\n",
       "      <td>3482</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>3304</td>\n",
       "      <td>3483</td>\n",
       "      <td>3366</td>\n",
       "      <td>5398</td>\n",
       "      <td>3304</td>\n",
       "      <td>3483</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4</td>\n",
       "      <td>3305</td>\n",
       "      <td>3484</td>\n",
       "      <td>3307</td>\n",
       "      <td>3486</td>\n",
       "      <td>5306</td>\n",
       "      <td>7189</td>\n",
       "      <td>3307</td>\n",
       "      <td>3486</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5</td>\n",
       "      <td>3306</td>\n",
       "      <td>3485</td>\n",
       "      <td>5293</td>\n",
       "      <td>5399</td>\n",
       "      <td>3380</td>\n",
       "      <td>3703</td>\n",
       "      <td>5314</td>\n",
       "      <td>7760</td>\n",
       "      <td>3380</td>\n",
       "      <td>3703</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6</td>\n",
       "      <td>3307</td>\n",
       "      <td>3486</td>\n",
       "      <td>5294</td>\n",
       "      <td>5400</td>\n",
       "      <td>3355</td>\n",
       "      <td>3486</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   col_0  col_1  col_2  col_3  col_4  col_5  col_6  col_7  col_8  col_9  \\\n",
       "0      2   3303   3482   5292   5397   3303   3482      0      0      0   \n",
       "1      3   3304   3483   3366   5398   3304   3483      0      0      0   \n",
       "2      4   3305   3484   3307   3486   5306   7189   3307   3486      0   \n",
       "3      5   3306   3485   5293   5399   3380   3703   5314   7760   3380   \n",
       "4      6   3307   3486   5294   5400   3355   3486      0      0      0   \n",
       "\n",
       "   col_10  col_11  \n",
       "0       0       0  \n",
       "1       0       0  \n",
       "2       0       0  \n",
       "3    3703       0  \n",
       "4       0       0  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Word-by-Word Tokenized DataFrame:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word_0</th>\n",
       "      <th>word_1</th>\n",
       "      <th>word_2</th>\n",
       "      <th>word_3</th>\n",
       "      <th>word_4</th>\n",
       "      <th>word_5</th>\n",
       "      <th>word_6</th>\n",
       "      <th>word_7</th>\n",
       "      <th>word_8</th>\n",
       "      <th>word_9</th>\n",
       "      <th>...</th>\n",
       "      <th>word_178</th>\n",
       "      <th>word_179</th>\n",
       "      <th>word_180</th>\n",
       "      <th>word_181</th>\n",
       "      <th>word_182</th>\n",
       "      <th>word_183</th>\n",
       "      <th>word_184</th>\n",
       "      <th>word_185</th>\n",
       "      <th>word_186</th>\n",
       "      <th>word_187</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>5358</td>\n",
       "      <td>232</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6</td>\n",
       "      <td>7</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>5359</td>\n",
       "      <td>28</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>5425</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>9</td>\n",
       "      <td>10</td>\n",
       "      <td>11</td>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>5360</td>\n",
       "      <td>227</td>\n",
       "      <td>5361</td>\n",
       "      <td>5362</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>13</td>\n",
       "      <td>14</td>\n",
       "      <td>15</td>\n",
       "      <td>16</td>\n",
       "      <td>17</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>5363</td>\n",
       "      <td>5364</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>9</td>\n",
       "      <td>18</td>\n",
       "      <td>19</td>\n",
       "      <td>20</td>\n",
       "      <td>21</td>\n",
       "      <td>22</td>\n",
       "      <td>23</td>\n",
       "      <td>24</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 188 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   word_0  word_1  word_2  word_3  word_4  word_5  word_6  word_7  word_8  \\\n",
       "0       2       3       4       5       1       0    5358     232       1   \n",
       "1       6       7       8       1       0    5359      28       1       0   \n",
       "2       9      10      11      12       1       0    5360     227    5361   \n",
       "3      13      14      15      16      17       1       0    5363    5364   \n",
       "4       9      18      19      20      21      22      23      24       1   \n",
       "\n",
       "   word_9  ...  word_178  word_179  word_180  word_181  word_182  word_183  \\\n",
       "0       0  ...         0         0         0         0         0         0   \n",
       "1    5425  ...         0         0         0         0         0         0   \n",
       "2    5362  ...         0         0         0         0         0         0   \n",
       "3       1  ...         0         0         0         0         0         0   \n",
       "4       0  ...         0         0         0         0         0         0   \n",
       "\n",
       "   word_184  word_185  word_186  word_187  \n",
       "0         0         0         0         0  \n",
       "1         0         0         0         0  \n",
       "2         0         0         0         0  \n",
       "3         0         0         0         0  \n",
       "4         0         0         0         0  \n",
       "\n",
       "[5 rows x 188 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Node/Edge Vocabulary len:\n",
      "8693\n",
      "\n",
      "Word Vocabulary len:\n",
      "9762\n"
     ]
    }
   ],
   "source": [
    "df_original, df_node_edge, df_word_by_word, node_edge_vocab, word_vocab = read_and_tokenize(DATA)\n",
    "\n",
    "# Print a sample\n",
    "print(\"Original DataFrame:\")\n",
    "display(df_original.head())\n",
    "print(\"\\nNode/Edge Tokenized DataFrame:\")\n",
    "display(df_node_edge.head())\n",
    "print(\"\\nWord-by-Word Tokenized DataFrame:\")\n",
    "display(df_word_by_word.head())\n",
    "print(\"\\nNode/Edge Vocabulary len:\")\n",
    "print(len(node_edge_vocab))\n",
    "print(\"\\nWord Vocabulary len:\")\n",
    "print(len(word_vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "261b3b00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/1000] | Loss: 6.6279 | Acc: 0.1297\n",
      "Epoch [2/1000] | Loss: 4.7819 | Acc: 0.3365\n",
      "Epoch [3/1000] | Loss: 3.8475 | Acc: 0.4601\n",
      "Epoch [4/1000] | Loss: 3.3238 | Acc: 0.5094\n",
      "Epoch [5/1000] | Loss: 2.9389 | Acc: 0.5423\n",
      "Epoch [6/1000] | Loss: 2.6169 | Acc: 0.5610\n",
      "Epoch [7/1000] | Loss: 2.3297 | Acc: 0.5831\n",
      "Epoch [8/1000] | Loss: 2.0696 | Acc: 0.6002\n",
      "Epoch [9/1000] | Loss: 1.8296 | Acc: 0.6245\n",
      "Epoch [10/1000] | Loss: 1.6090 | Acc: 0.6454\n",
      "Epoch [11/1000] | Loss: 1.3885 | Acc: 0.6769\n",
      "Epoch [12/1000] | Loss: 1.2143 | Acc: 0.7051\n",
      "Epoch [13/1000] | Loss: 1.0506 | Acc: 0.7384\n",
      "Epoch [14/1000] | Loss: 0.9230 | Acc: 0.7692\n",
      "Epoch [15/1000] | Loss: 0.7838 | Acc: 0.8010\n",
      "Epoch [16/1000] | Loss: 0.6820 | Acc: 0.8231\n",
      "Epoch [17/1000] | Loss: 0.6110 | Acc: 0.8377\n",
      "Epoch [18/1000] | Loss: 0.5286 | Acc: 0.8590\n",
      "Epoch [19/1000] | Loss: 0.4973 | Acc: 0.8648\n",
      "Epoch [20/1000] | Loss: 0.4206 | Acc: 0.8868\n",
      "Epoch [21/1000] | Loss: 0.4075 | Acc: 0.8867\n",
      "Epoch [22/1000] | Loss: 0.3800 | Acc: 0.8948\n",
      "Epoch [23/1000] | Loss: 0.3622 | Acc: 0.9007\n",
      "Epoch [24/1000] | Loss: 0.3359 | Acc: 0.9080\n",
      "Epoch [25/1000] | Loss: 0.3153 | Acc: 0.9104\n",
      "Epoch [26/1000] | Loss: 0.3050 | Acc: 0.9123\n",
      "Epoch [27/1000] | Loss: 0.2712 | Acc: 0.9253\n",
      "Epoch [28/1000] | Loss: 0.2925 | Acc: 0.9160\n",
      "Epoch [29/1000] | Loss: 0.2537 | Acc: 0.9296\n",
      "Epoch [30/1000] | Loss: 0.2496 | Acc: 0.9296\n",
      "Epoch [31/1000] | Loss: 0.2559 | Acc: 0.9258\n",
      "Epoch [32/1000] | Loss: 0.2382 | Acc: 0.9316\n",
      "Epoch [33/1000] | Loss: 0.2475 | Acc: 0.9280\n",
      "Epoch [34/1000] | Loss: 0.2333 | Acc: 0.9342\n",
      "Epoch [35/1000] | Loss: 0.2321 | Acc: 0.9338\n",
      "Epoch [36/1000] | Loss: 0.2188 | Acc: 0.9356\n",
      "Epoch [37/1000] | Loss: 0.2186 | Acc: 0.9364\n",
      "Epoch [38/1000] | Loss: 0.2276 | Acc: 0.9351\n",
      "Epoch [39/1000] | Loss: 0.2239 | Acc: 0.9337\n",
      "Epoch [40/1000] | Loss: 0.2013 | Acc: 0.9412\n",
      "Epoch [41/1000] | Loss: 0.2159 | Acc: 0.9376\n",
      "Epoch [42/1000] | Loss: 0.2042 | Acc: 0.9407\n",
      "Epoch [43/1000] | Loss: 0.2065 | Acc: 0.9391\n",
      "Epoch [44/1000] | Loss: 0.1910 | Acc: 0.9450\n",
      "Epoch [45/1000] | Loss: 0.1986 | Acc: 0.9425\n",
      "Epoch [46/1000] | Loss: 0.1890 | Acc: 0.9441\n",
      "Epoch [47/1000] | Loss: 0.1843 | Acc: 0.9438\n",
      "Epoch [48/1000] | Loss: 0.1738 | Acc: 0.9502\n",
      "Epoch [49/1000] | Loss: 0.1819 | Acc: 0.9475\n",
      "Epoch [50/1000] | Loss: 0.1813 | Acc: 0.9465\n",
      "Epoch [51/1000] | Loss: 0.1846 | Acc: 0.9483\n",
      "Epoch [52/1000] | Loss: 0.1783 | Acc: 0.9460\n",
      "Epoch [53/1000] | Loss: 0.1684 | Acc: 0.9504\n",
      "Epoch [54/1000] | Loss: 0.1573 | Acc: 0.9537\n",
      "Epoch [55/1000] | Loss: 0.1806 | Acc: 0.9464\n",
      "Epoch [56/1000] | Loss: 0.1700 | Acc: 0.9508\n",
      "Epoch [57/1000] | Loss: 0.1602 | Acc: 0.9530\n",
      "Epoch [58/1000] | Loss: 0.1538 | Acc: 0.9556\n",
      "Epoch [59/1000] | Loss: 0.1683 | Acc: 0.9504\n",
      "Epoch [60/1000] | Loss: 0.1736 | Acc: 0.9511\n",
      "Epoch [61/1000] | Loss: 0.1674 | Acc: 0.9509\n",
      "Epoch [62/1000] | Loss: 0.1566 | Acc: 0.9537\n",
      "Epoch [63/1000] | Loss: 0.1488 | Acc: 0.9567\n",
      "Epoch [64/1000] | Loss: 0.1603 | Acc: 0.9525\n",
      "Epoch [65/1000] | Loss: 0.1567 | Acc: 0.9536\n",
      "Epoch [66/1000] | Loss: 0.1721 | Acc: 0.9481\n",
      "Epoch [67/1000] | Loss: 0.1543 | Acc: 0.9531\n",
      "Epoch [68/1000] | Loss: 0.1419 | Acc: 0.9564\n",
      "Epoch [69/1000] | Loss: 0.1560 | Acc: 0.9550\n",
      "Epoch [70/1000] | Loss: 0.1525 | Acc: 0.9541\n",
      "Epoch [71/1000] | Loss: 0.1558 | Acc: 0.9561\n",
      "Epoch [72/1000] | Loss: 0.1595 | Acc: 0.9544\n",
      "Epoch [73/1000] | Loss: 0.1438 | Acc: 0.9588\n",
      "Epoch [74/1000] | Loss: 0.1381 | Acc: 0.9606\n",
      "Epoch [75/1000] | Loss: 0.1412 | Acc: 0.9574\n",
      "Epoch [76/1000] | Loss: 0.1403 | Acc: 0.9585\n",
      "Epoch [77/1000] | Loss: 0.1635 | Acc: 0.9533\n",
      "Epoch [78/1000] | Loss: 0.1420 | Acc: 0.9567\n",
      "Epoch [79/1000] | Loss: 0.1436 | Acc: 0.9584\n",
      "Epoch [80/1000] | Loss: 0.1563 | Acc: 0.9538\n",
      "Epoch [81/1000] | Loss: 0.1455 | Acc: 0.9575\n",
      "Epoch [82/1000] | Loss: 0.1397 | Acc: 0.9580\n",
      "Epoch [83/1000] | Loss: 0.1265 | Acc: 0.9618\n",
      "Epoch [84/1000] | Loss: 0.1239 | Acc: 0.9627\n",
      "Epoch [85/1000] | Loss: 0.1371 | Acc: 0.9576\n",
      "Epoch [86/1000] | Loss: 0.1375 | Acc: 0.9590\n",
      "Epoch [87/1000] | Loss: 0.1381 | Acc: 0.9598\n",
      "Epoch [88/1000] | Loss: 0.1510 | Acc: 0.9573\n",
      "Epoch [89/1000] | Loss: 0.1295 | Acc: 0.9631\n",
      "Epoch [90/1000] | Loss: 0.1271 | Acc: 0.9622\n",
      "Epoch [91/1000] | Loss: 0.1276 | Acc: 0.9602\n",
      "Epoch [92/1000] | Loss: 0.1476 | Acc: 0.9578\n",
      "Epoch [93/1000] | Loss: 0.1344 | Acc: 0.9598\n",
      "Epoch [94/1000] | Loss: 0.1314 | Acc: 0.9618\n",
      "Epoch [95/1000] | Loss: 0.1281 | Acc: 0.9609\n",
      "Epoch [96/1000] | Loss: 0.1323 | Acc: 0.9625\n",
      "Epoch [97/1000] | Loss: 0.1168 | Acc: 0.9645\n",
      "Epoch [98/1000] | Loss: 0.1302 | Acc: 0.9606\n",
      "Epoch [99/1000] | Loss: 0.1453 | Acc: 0.9594\n",
      "Epoch [100/1000] | Loss: 0.1241 | Acc: 0.9634\n",
      "Epoch [101/1000] | Loss: 0.1404 | Acc: 0.9597\n",
      "Epoch [102/1000] | Loss: 0.1283 | Acc: 0.9625\n",
      "Epoch [103/1000] | Loss: 0.1252 | Acc: 0.9626\n",
      "Epoch [104/1000] | Loss: 0.1337 | Acc: 0.9618\n",
      "Epoch [105/1000] | Loss: 0.1244 | Acc: 0.9629\n",
      "Epoch [106/1000] | Loss: 0.1236 | Acc: 0.9629\n",
      "Epoch [107/1000] | Loss: 0.1347 | Acc: 0.9618\n",
      "Epoch [108/1000] | Loss: 0.1206 | Acc: 0.9639\n",
      "Epoch [109/1000] | Loss: 0.1170 | Acc: 0.9640\n",
      "Epoch [110/1000] | Loss: 0.1255 | Acc: 0.9623\n",
      "Epoch [111/1000] | Loss: 0.1246 | Acc: 0.9640\n",
      "Epoch [112/1000] | Loss: 0.1139 | Acc: 0.9659\n",
      "Epoch [113/1000] | Loss: 0.1097 | Acc: 0.9684\n",
      "Epoch [114/1000] | Loss: 0.1128 | Acc: 0.9649\n",
      "Epoch [115/1000] | Loss: 0.1334 | Acc: 0.9628\n",
      "Epoch [116/1000] | Loss: 0.1320 | Acc: 0.9622\n",
      "Epoch [117/1000] | Loss: 0.1299 | Acc: 0.9639\n",
      "Epoch [118/1000] | Loss: 0.1205 | Acc: 0.9661\n",
      "Epoch [119/1000] | Loss: 0.1174 | Acc: 0.9665\n",
      "Epoch [120/1000] | Loss: 0.1106 | Acc: 0.9667\n",
      "Epoch [121/1000] | Loss: 0.1183 | Acc: 0.9650\n",
      "Epoch [122/1000] | Loss: 0.1156 | Acc: 0.9653\n",
      "Epoch [123/1000] | Loss: 0.1146 | Acc: 0.9664\n",
      "Epoch [124/1000] | Loss: 0.1286 | Acc: 0.9631\n",
      "Epoch [125/1000] | Loss: 0.1128 | Acc: 0.9674\n",
      "Epoch [126/1000] | Loss: 0.1147 | Acc: 0.9654\n",
      "Epoch [127/1000] | Loss: 0.1115 | Acc: 0.9671\n",
      "Epoch [128/1000] | Loss: 0.1100 | Acc: 0.9674\n",
      "Epoch [129/1000] | Loss: 0.1221 | Acc: 0.9652\n",
      "Epoch [130/1000] | Loss: 0.1105 | Acc: 0.9669\n",
      "Epoch [131/1000] | Loss: 0.1074 | Acc: 0.9687\n",
      "Epoch [132/1000] | Loss: 0.1076 | Acc: 0.9673\n",
      "Epoch [133/1000] | Loss: 0.1129 | Acc: 0.9670\n",
      "Epoch [134/1000] | Loss: 0.1380 | Acc: 0.9630\n",
      "Epoch [135/1000] | Loss: 0.1059 | Acc: 0.9690\n",
      "Epoch [136/1000] | Loss: 0.1183 | Acc: 0.9644\n",
      "Epoch [137/1000] | Loss: 0.1101 | Acc: 0.9670\n",
      "Epoch [138/1000] | Loss: 0.0970 | Acc: 0.9717\n",
      "Epoch [139/1000] | Loss: 0.1191 | Acc: 0.9662\n",
      "Epoch [140/1000] | Loss: 0.1137 | Acc: 0.9645\n",
      "Epoch [141/1000] | Loss: 0.1008 | Acc: 0.9690\n",
      "Epoch [142/1000] | Loss: 0.0987 | Acc: 0.9711\n",
      "Epoch [143/1000] | Loss: 0.1157 | Acc: 0.9675\n",
      "Epoch [144/1000] | Loss: 0.1088 | Acc: 0.9693\n",
      "Epoch [145/1000] | Loss: 0.1139 | Acc: 0.9654\n",
      "Epoch [146/1000] | Loss: 0.1026 | Acc: 0.9693\n",
      "Epoch [147/1000] | Loss: 0.1083 | Acc: 0.9675\n",
      "Epoch [148/1000] | Loss: 0.1078 | Acc: 0.9675\n",
      "Epoch [149/1000] | Loss: 0.1148 | Acc: 0.9672\n",
      "Epoch [150/1000] | Loss: 0.1035 | Acc: 0.9696\n",
      "Epoch [151/1000] | Loss: 0.1030 | Acc: 0.9697\n",
      "Epoch [152/1000] | Loss: 0.1141 | Acc: 0.9676\n",
      "Epoch [153/1000] | Loss: 0.1158 | Acc: 0.9661\n",
      "Epoch [154/1000] | Loss: 0.1162 | Acc: 0.9657\n",
      "Epoch [155/1000] | Loss: 0.1109 | Acc: 0.9675\n",
      "Epoch [156/1000] | Loss: 0.1068 | Acc: 0.9693\n",
      "Epoch [157/1000] | Loss: 0.1069 | Acc: 0.9659\n",
      "Epoch [158/1000] | Loss: 0.1024 | Acc: 0.9686\n",
      "Epoch [159/1000] | Loss: 0.1119 | Acc: 0.9684\n",
      "Epoch [160/1000] | Loss: 0.1026 | Acc: 0.9699\n",
      "Epoch [161/1000] | Loss: 0.1025 | Acc: 0.9702\n",
      "Epoch [162/1000] | Loss: 0.1077 | Acc: 0.9679\n",
      "Epoch [163/1000] | Loss: 0.0971 | Acc: 0.9718\n",
      "Epoch [164/1000] | Loss: 0.0998 | Acc: 0.9711\n",
      "Epoch [165/1000] | Loss: 0.1086 | Acc: 0.9670\n",
      "Epoch [166/1000] | Loss: 0.0978 | Acc: 0.9717\n",
      "Epoch [167/1000] | Loss: 0.0964 | Acc: 0.9709\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[127], line 231\u001b[0m\n\u001b[1;32m    220\u001b[0m model \u001b[38;5;241m=\u001b[39m GPTLikeModel(\n\u001b[1;32m    221\u001b[0m     vocab_size\u001b[38;5;241m=\u001b[39mvocab_size,\n\u001b[1;32m    222\u001b[0m     d_model\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m32\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    227\u001b[0m     activation_fn\u001b[38;5;241m=\u001b[39mnn\u001b[38;5;241m.\u001b[39mGELU()\n\u001b[1;32m    228\u001b[0m )\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m    230\u001b[0m \u001b[38;5;66;03m# Train\u001b[39;00m\n\u001b[0;32m--> 231\u001b[0m train_model(\n\u001b[1;32m    232\u001b[0m     model,\n\u001b[1;32m    233\u001b[0m     data_loader\u001b[38;5;241m=\u001b[39mdata_loader,\n\u001b[1;32m    234\u001b[0m     epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1000\u001b[39m,\n\u001b[1;32m    235\u001b[0m     lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1e-3\u001b[39m,\n\u001b[1;32m    236\u001b[0m     pad_token\u001b[38;5;241m=\u001b[39mpad_token,\n\u001b[1;32m    237\u001b[0m     device\u001b[38;5;241m=\u001b[39mdevice\n\u001b[1;32m    238\u001b[0m )\n",
      "Cell \u001b[0;32mIn[127], line 198\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m(model, data_loader, epochs, lr, pad_token, device)\u001b[0m\n\u001b[1;32m    195\u001b[0m criterion \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mCrossEntropyLoss(ignore_index\u001b[38;5;241m=\u001b[39mpad_token)\n\u001b[1;32m    197\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m e \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, epochs \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m):\n\u001b[0;32m--> 198\u001b[0m     loss, acc \u001b[38;5;241m=\u001b[39m train_one_epoch(model, data_loader, optimizer, criterion, device, pad_token)\n\u001b[1;32m    199\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m] | Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mloss\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m | Acc: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00macc\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[127], line 167\u001b[0m, in \u001b[0;36mtrain_one_epoch\u001b[0;34m(model, data_loader, optimizer, criterion, device, pad_token)\u001b[0m\n\u001b[1;32m    165\u001b[0m \u001b[38;5;66;03m# Backprop\u001b[39;00m\n\u001b[1;32m    166\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m--> 167\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m    168\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m    170\u001b[0m \u001b[38;5;66;03m# Accumulate metrics\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/_tensor.py:492\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    482\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    483\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    484\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    485\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    490\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    491\u001b[0m     )\n\u001b[0;32m--> 492\u001b[0m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mbackward(\n\u001b[1;32m    493\u001b[0m     \u001b[38;5;28mself\u001b[39m, gradient, retain_graph, create_graph, inputs\u001b[38;5;241m=\u001b[39minputs\n\u001b[1;32m    494\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/autograd/__init__.py:251\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    246\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    248\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    249\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    250\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 251\u001b[0m Variable\u001b[38;5;241m.\u001b[39m_execution_engine\u001b[38;5;241m.\u001b[39mrun_backward(  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    252\u001b[0m     tensors,\n\u001b[1;32m    253\u001b[0m     grad_tensors_,\n\u001b[1;32m    254\u001b[0m     retain_graph,\n\u001b[1;32m    255\u001b[0m     create_graph,\n\u001b[1;32m    256\u001b[0m     inputs,\n\u001b[1;32m    257\u001b[0m     allow_unreachable\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    258\u001b[0m     accumulate_grad\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    259\u001b[0m )\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm import tqdm\n",
    "\n",
    "##########################################\n",
    "# 1) Simple Toy Dataset\n",
    "##########################################\n",
    "class SimpleSequenceDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Example dataset that returns integer sequences of various lengths,\n",
    "    with 0 as the PAD token.\n",
    "    \"\"\"\n",
    "    def __init__(self, sequences, pad_token=0):\n",
    "        self.sequences = sequences\n",
    "        self.pad_token = pad_token\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sequences)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return torch.tensor(self.sequences[idx], dtype=torch.long)\n",
    "\n",
    "def collate_fn(batch, pad_token=0):\n",
    "    \"\"\"\n",
    "    Collate function to pad a list of variable-length sequences\n",
    "    so that each batch is shape [batch_size, seq_len].\n",
    "    \"\"\"\n",
    "    lengths = [len(seq) for seq in batch]\n",
    "    max_len = max(lengths)\n",
    "    padded = []\n",
    "    for seq in batch:\n",
    "        pad_size = max_len - len(seq)\n",
    "        padded.append(torch.cat([seq, torch.full((pad_size,), pad_token, dtype=torch.long)]))\n",
    "    return torch.stack(padded)  # shape = (batch_size, max_len)\n",
    "\n",
    "##########################################\n",
    "# 2) Causal Transformer Model\n",
    "##########################################\n",
    "class GPTLikeModel(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        vocab_size,\n",
    "        d_model=128,\n",
    "        n_heads=4,\n",
    "        n_layers=2,\n",
    "        max_seq_len=50,\n",
    "        pad_token=0,\n",
    "        activation_fn=nn.GELU,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.pad_token = pad_token\n",
    "        self.d_model = d_model\n",
    "        self.max_seq_len = max_seq_len\n",
    "\n",
    "        self.embedding = nn.Embedding(vocab_size, d_model, padding_idx=pad_token)\n",
    "        # A simple trainable positional encoding table:\n",
    "        self.pos_emb = nn.Embedding(max_seq_len, d_model)\n",
    "\n",
    "        # Transformer layers in \"decoder\" style:\n",
    "        self.transformer_layers = nn.ModuleList([\n",
    "            nn.TransformerDecoderLayer(\n",
    "                d_model=d_model,\n",
    "                nhead=n_heads,\n",
    "                activation=activation_fn,\n",
    "                batch_first=True  # <--- crucial so we can feed (B, T, E)\n",
    "            ) for _ in range(n_layers)\n",
    "        ])\n",
    "\n",
    "        self.fc_out = nn.Linear(d_model, vocab_size)\n",
    "\n",
    "    def forward(self, x, causal_mask=None, key_padding_mask=None):\n",
    "        \"\"\"\n",
    "        x shape: [batch_size, seq_len]\n",
    "        causal_mask shape: [seq_len, seq_len] if batch_first=True\n",
    "        key_padding_mask shape: [batch_size, seq_len]\n",
    "        \"\"\"\n",
    "        batch_size, seq_len = x.shape\n",
    "        \n",
    "        # Embedding + (optional) positional encoding\n",
    "        positions = torch.arange(seq_len, device=x.device).unsqueeze(0)  # [1, seq_len]\n",
    "        # shape => (batch_size, seq_len, d_model)\n",
    "        x = self.embedding(x) + self.pos_emb(positions)\n",
    "\n",
    "        # Pass through each TransformerDecoderLayer\n",
    "        # For a \"GPT-like\" approach, we typically feed the same x as both \"tgt\" and \"memory\"\n",
    "        out = x\n",
    "        for layer in self.transformer_layers:\n",
    "            out = layer(\n",
    "                tgt=out,\n",
    "                memory=out,\n",
    "                tgt_mask=causal_mask,                   # (seq_len, seq_len)\n",
    "                memory_mask=None,                       # not needed for GPT-like\n",
    "                tgt_key_padding_mask=key_padding_mask,   # (batch_size, seq_len)\n",
    "                memory_key_padding_mask=key_padding_mask # same as above in GPT-like\n",
    "            )\n",
    "\n",
    "        logits = self.fc_out(out)  # [batch_size, seq_len, vocab_size]\n",
    "        return logits\n",
    "\n",
    "##########################################\n",
    "# 3) Utilities: Generate Causal Mask\n",
    "##########################################\n",
    "def generate_causal_mask(seq_len, device):\n",
    "    \"\"\"\n",
    "    Generates a causal mask for self-attention (upper-triangular True).\n",
    "    If using batch_first=True, shape must be [seq_len, seq_len].\n",
    "    True entries in the mask indicate positions that should be masked (blocked).\n",
    "    \"\"\"\n",
    "    # shape: (seq_len, seq_len), True means \"do not attend\".\n",
    "    mask = torch.triu(torch.ones(seq_len, seq_len, device=device), diagonal=1).bool()\n",
    "    return mask\n",
    "\n",
    "##########################################\n",
    "# 4) Training Function\n",
    "##########################################\n",
    "def train_one_epoch(\n",
    "    model, \n",
    "    data_loader, \n",
    "    optimizer, \n",
    "    criterion, \n",
    "    device, \n",
    "    pad_token=0\n",
    "):\n",
    "    model.train()\n",
    "\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total_tokens = 0\n",
    "\n",
    "    for batch in data_loader:\n",
    "        # batch shape: [batch_size, seq_len]\n",
    "        batch = batch.to(device)\n",
    "        batch_size, seq_len = batch.shape\n",
    "\n",
    "        # We do next-token prediction:\n",
    "        #   Input  = batch[:, :-1]\n",
    "        #   Target = batch[:, 1:]\n",
    "        #   So the output should be shape (batch_size, seq_len-1, vocab_size)\n",
    "        #   We'll generate a (seq_len-1, seq_len-1) causal mask\n",
    "        #   Also we shift the key_padding_mask to exclude the last token\n",
    "        if seq_len < 2:\n",
    "            # If any sequence < 2 tokens, skip\n",
    "            continue\n",
    "        inp = batch[:, :-1]   # shape (B, T-1)\n",
    "        tgt = batch[:, 1:]    # shape (B, T-1)\n",
    "\n",
    "        # Prepare mask\n",
    "        c_mask = generate_causal_mask(seq_len - 1, device=device)  # shape (T-1, T-1)\n",
    "        # Key padding mask: True where PAD => (B, T-1)\n",
    "        kp_mask = (inp == pad_token)\n",
    "\n",
    "        # Forward pass\n",
    "        logits = model(inp, causal_mask=c_mask, key_padding_mask=kp_mask)\n",
    "        # logits shape = [batch_size, seq_len-1, vocab_size]\n",
    "        \n",
    "        # Flatten for cross-entropy\n",
    "        logits_2d = logits.reshape(-1, logits.size(-1))   # [B*(T-1), vocab_size]\n",
    "        tgt_1d    = tgt.reshape(-1)                       # [B*(T-1)]\n",
    "\n",
    "        # Compute loss\n",
    "        loss = criterion(logits_2d, tgt_1d)\n",
    "\n",
    "        # Backprop\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Accumulate metrics\n",
    "        total_loss += loss.item() * batch_size\n",
    "\n",
    "        # Accuracy: ignore pad positions\n",
    "        #  (pred == tgt) & (tgt != pad_token)\n",
    "        preds = logits_2d.argmax(dim=-1)  # [B*(T-1)]\n",
    "        valid_mask = (tgt_1d != pad_token)\n",
    "        correct += (preds[valid_mask] == tgt_1d[valid_mask]).sum().item()\n",
    "        total_tokens += valid_mask.sum().item()\n",
    "\n",
    "    avg_loss = total_loss / len(data_loader.dataset)\n",
    "    avg_acc = correct / (total_tokens + 1e-9)\n",
    "    return avg_loss, avg_acc\n",
    "\n",
    "\n",
    "def train_model(\n",
    "    model,\n",
    "    data_loader,\n",
    "    epochs,\n",
    "    lr=1e-3,\n",
    "    pad_token=0,\n",
    "    device='cpu'\n",
    "):\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    # CrossEntropy with ignore_index = pad_token\n",
    "    criterion = nn.CrossEntropyLoss(ignore_index=pad_token)\n",
    "\n",
    "    for e in range(1, epochs + 1):\n",
    "        loss, acc = train_one_epoch(model, data_loader, optimizer, criterion, device, pad_token)\n",
    "        print(f\"Epoch [{e}/{epochs}] | Loss: {loss:.4f} | Acc: {acc:.4f}\")\n",
    "\n",
    "\n",
    "##########################################\n",
    "# 5) Example Usage\n",
    "##########################################\n",
    "if __name__ == \"__main__\":\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    # Example 'vocab_size' includes pad_token=0 up to some max token\n",
    "    vocab_size = len(node_edge_vocab)+1\n",
    "    pad_token = 0\n",
    "\n",
    "    # Let's create random sequences of length up to 8\n",
    "    # Real code: you can supply your own sequences from df_node_edge, etc.\n",
    "    random_data = df_node_edge.to_numpy()\n",
    "\n",
    "    dataset = SimpleSequenceDataset(random_data, pad_token=pad_token)\n",
    "    data_loader = DataLoader(dataset, batch_size=4, shuffle=True, collate_fn=lambda b: collate_fn(b, pad_token))\n",
    "\n",
    "    # Build model\n",
    "    model = GPTLikeModel(\n",
    "        vocab_size=vocab_size,\n",
    "        d_model=32,\n",
    "        n_heads=4,\n",
    "        n_layers=2,\n",
    "        max_seq_len=50,   # can be bigger than any real seq\n",
    "        pad_token=pad_token,\n",
    "        activation_fn=nn.GELU()\n",
    "    ).to(device)\n",
    "\n",
    "    # Train\n",
    "    train_model(\n",
    "        model,\n",
    "        data_loader=data_loader,\n",
    "        epochs=1000,\n",
    "        lr=1e-3,\n",
    "        pad_token=pad_token,\n",
    "        device=device\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad76f394",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
